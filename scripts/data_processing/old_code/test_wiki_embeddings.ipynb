{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test wiki embeddings\n",
    "In which we test the effectiveness of embeddings trained on Wikipedia \"words, entities and concepts\", i.e. ConVec (described [here](https://github.com/ehsansherkat/ConVec) and in [Sherkat and Milios, 2017](https://arxiv.org/pdf/1702.03470.pdf))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "wiki_embeddings = pd.read_csv('/hg190/corpora/wiki_embeddings/ConVec/WikipediaClean5Negative300Skip10.txt', sep=' ', \n",
    "                              index_col=0, header=None, skiprows=1)\n",
    "# drop nan index??\n",
    "print(wiki_embeddings.drop(pd.np.nan, axis=0, inplace=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Test neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we find that entities have reasonable nearest neighbors? E.g., \"Obama\" could be similar to \"Clinton\" or \"Bush\" (depending on who's writing the articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def get_neighbors(word, embeddings, top_k=10, dist='cosine'):\n",
    "    # compute similarities\n",
    "    w_vec = embeddings.loc[word, :].values.reshape(1,-1)\n",
    "    word_sims = pd.Series(cdist(embeddings, w_vec, dist).reshape(-1), index=embeddings.index)\n",
    "    word_sims.sort_values(inplace=True, ascending=True)\n",
    "    word_sims = word_sims[1:top_k+1]\n",
    "    return word_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 neighbors of obama: 534366,barack,obamas,3414021,3356,biden,20082093,145422,5043192,5122699\n",
      "top 10 neighbors of tomato: potato,eggplant,onions,9940234,carrots,garlic,spinach,lettuce,potatoes,onion\n",
      "top 10 neighbors of water: seawater,potable,groundwater,seepage,198725,wastewater,262927,sewage,waters,desalinated\n",
      "top 10 neighbors of atlanta: 3138,memphis,nashville,peachtree,dallas,miami,houston,knoxville,jacksonville,louisville\n",
      "top 10 neighbors of georgia: tennessee,alabama,georgias,florida,carolina,virginia,kentucky,arkansas,8733443,atlanta\n",
      "top 10 neighbors of usa: america,cinematexas,iscp,canada,united,germany,41194889,goteberg,cologneoff,miniprint\n"
     ]
    }
   ],
   "source": [
    "test_words = ['obama', 'tomato', 'water', 'atlanta', 'georgia', 'usa']\n",
    "top_k = 10\n",
    "for w in test_words:\n",
    "    neighbors = get_neighbors(w, wiki_embeddings, top_k=top_k)\n",
    "    print('top %d neighbors of %s: %s'%(top_k, w, ','.join(neighbors.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of these make sense: `Atlanta` has other capital cities (of Southern states) as neighbors.\n",
    "\n",
    "There's a lot of numbers in here! I guess they are IDs for specific entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16527332 total entities\n",
      "10     AccessibleComputing\n",
      "12               Anarchism\n",
      "13      AfghanistanHistory\n",
      "14    AfghanistanGeography\n",
      "15       AfghanistanPeople\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "id_title_file = '/hg190/corpora/wiki_embeddings/ConVec/id_title_map.csv'\n",
    "# get rid of first column delimited by colon for some reason\n",
    "# and convert second column ID to integer\n",
    "id_title_lookup = [l.strip().split(',') for l in codecs.open(id_title_file, 'r', encoding='utf-8') if len(l.strip().split(',')) >= 2]\n",
    "id_title_lookup = [(int(i[0].split(':')[1]), ','.join(i[1:])) for i in id_title_lookup]\n",
    "id_title_lookup = dict(id_title_lookup)\n",
    "id_title_lookup = pd.Series(id_title_lookup)\n",
    "print('%d total entities'%(len(id_title_lookup)))\n",
    "print(id_title_lookup.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's replace all the number IDs in the embeddings with their entity equivalent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from itertools import izip\n",
    "updated_idx = []\n",
    "entity_suffix = '_ENTITY'\n",
    "for i in wiki_embeddings.index:\n",
    "    if(i.isdigit()):\n",
    "        try:\n",
    "            i_int = int(i)\n",
    "            if(i_int in id_title_lookup.index):\n",
    "                new_idx = id_title_lookup.loc[i_int]\n",
    "                # add suffix to indicate entity\n",
    "                new_idx += entity_suffix\n",
    "                updated_idx.append(new_idx)\n",
    "            else:\n",
    "                updated_idx.append(i)\n",
    "        # exception because pandas can't handle long indices\n",
    "        except Exception, e:\n",
    "            updated_idx.append(i)\n",
    "    else:\n",
    "        updated_idx.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And do the same test as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'! (The Dismemberment Plan album)_ENTITY', u'!!!_ENTITY',\n",
      "       u'!Action Pact!_ENTITY', u'!Hero (album)_ENTITY', u'!Hero_ENTITY',\n",
      "       u'!Kung language_ENTITY', u'!Oka Tokat_ENTITY', u'!T.O.O.H.!_ENTITY',\n",
      "       u'!WOWOW!_ENTITY', u'!Women Art Revolution_ENTITY'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "wiki_embeddings_full = wiki_embeddings.copy()\n",
    "wiki_embeddings_full.index = updated_idx\n",
    "print(wiki_embeddings_full.index.sort_values()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 neighbors of usa: america;cinematexas;iscp;canada;united;germany;Art Palm Beach_ENTITY;goteberg;cologneoff;miniprint\n",
      "top 10 neighbors of obama: Barack Obama_ENTITY;barack;obamas;George W. Bush_ENTITY;Bill Clinton_ENTITY;biden;Presidency of Barack Obama_ENTITY;Joe Biden_ENTITY;Hillary Clinton_ENTITY;John Kerry_ENTITY\n",
      "top 10 neighbors of tomato: potato;eggplant;onions;Tomato_ENTITY;carrots;garlic;spinach;lettuce;potatoes;onion\n",
      "top 10 neighbors of water: seawater;potable;groundwater;seepage;Drinking water_ENTITY;wastewater;Groundwater_ENTITY;sewage;waters;desalinated\n",
      "top 10 neighbors of Atlanta_ENTITY: Decatur, Georgia_ENTITY;atlanta;Marietta, Georgia_ENTITY;Savannah, Georgia_ENTITY;Macon, Georgia_ENTITY;Nashville, Tennessee_ENTITY;Memphis, Tennessee_ENTITY;Charlotte, North Carolina_ENTITY;Birmingham, Alabama_ENTITY;Georgia (U.S. state)_ENTITY\n",
      "top 10 neighbors of georgia: tennessee;alabama;georgias;florida;carolina;virginia;kentucky;arkansas;Herty Field_ENTITY;atlanta\n"
     ]
    }
   ],
   "source": [
    "test_words = ['usa', 'obama', 'tomato', 'water', 'Atlanta_ENTITY', 'georgia']\n",
    "top_k = 10\n",
    "for w in test_words:\n",
    "    neighbors = get_neighbors(w, wiki_embeddings_full, top_k=top_k)\n",
    "    print('top %d neighbors of %s: %s'%(top_k, w, ';'.join(neighbors.index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Better! Although we still see some overlap in names due to capitalization errors (`atlanta` vs. `Atlanta`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save these updated embeddings for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "from itertools import izip\n",
    "wiki_full_file_name = '/hg190/corpora/wiki_embeddings/ConVec/WikipediaClean5Negative300Skip10_withentities.gz'\n",
    "with gzip.open(wiki_full_file_name, 'w') as wiki_full_file:\n",
    "    for i, r in wiki_embeddings_full.iterrows():\n",
    "        r_combined = [i] + map(str, r.values.tolist())\n",
    "        r_combined_str = '\\t'.join(r_combined).encode('utf-8')\n",
    "        wiki_full_file.write('%s\\n'%(r_combined_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
