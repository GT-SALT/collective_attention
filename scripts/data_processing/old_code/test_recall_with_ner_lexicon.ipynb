{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test recall with NER and lexicon\n",
    "In which we determine which method for entity candidate generation will provide better recall. Maybe they're both terrible!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import data_helpers\n",
    "reload(data_helpers)\n",
    "from data_helpers import get_mention_entity_lists\n",
    "import os\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Let's pick data from one of the more active Facebook groups, based on total post volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 istewart6 gtperson 1.8M Oct 25 17:58 ../../data/facebook-maria/1773209126315380_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 1.3M Oct 21 19:44 ../../data/facebook-maria/296720367472342_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 1.1M Oct 25 17:54 ../../data/facebook-maria/132392547395104_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 1.1M Oct 25 17:54 ../../data/facebook-maria/127217134598253_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 1.1M Oct 20 19:38 ../../data/facebook-maria/296720367472342_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 849K Oct 23 17:56 ../../data/facebook-maria/274834366346071_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 753K Oct 25 17:53 ../../data/facebook-maria/891721064308258_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 647K Oct 25 17:52 ../../data/facebook-maria/119087272129297_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 629K Oct 25 17:52 ../../data/facebook-maria/1979604895658060_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 516K Oct 26 17:56 ../../data/facebook-maria/130913387550000_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 464K Oct 25 17:51 ../../data/facebook-maria/117445378946554_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 457K Oct 25 17:51 ../../data/facebook-maria/130913387550000_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 452K Oct 26 17:56 ../../data/facebook-maria/127217134598253_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 444K Oct 25 17:52 ../../data/facebook-maria/132963337341347_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 432K Oct 26 17:56 ../../data/facebook-maria/891721064308258_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 422K Oct 25 17:51 ../../data/facebook-maria/721270821393500_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 412K Oct 26 17:56 ../../data/facebook-maria/132963337341347_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 410K Oct 22 15:14 ../../data/facebook-maria/171400810082867_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 376K Oct 26 17:56 ../../data/facebook-maria/844368579056187_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 374K Oct 25 17:51 ../../data/facebook-maria/228066954387757_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 360K Oct 25 17:51 ../../data/facebook-maria/1815374475420954_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 356K Oct 26 17:56 ../../data/facebook-maria/132022090761950_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 352K Oct 26 17:56 ../../data/facebook-maria/117445378946554_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 336K Oct 26 17:56 ../../data/facebook-maria/1972572999632293_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 312K Oct 26 17:56 ../../data/facebook-maria/1723084327997466_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 303K Oct 25 17:50 ../../data/facebook-maria/132022090761950_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 286K Oct 25 17:50 ../../data/facebook-maria/1972572999632293_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 280K Oct 26 17:56 ../../data/facebook-maria/320205998443003_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 276K Oct 26 17:55 ../../data/facebook-maria/1815374475420954_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 276K Oct 26 17:56 ../../data/facebook-maria/239379013253824_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 263K Oct 25 17:51 ../../data/facebook-maria/239379013253824_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 256K Oct 26 17:56 ../../data/facebook-maria/118787682128186_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 255K Oct 25 17:50 ../../data/facebook-maria/311739902631820_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 245K Oct 25 17:50 ../../data/facebook-maria/1723084327997466_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 238K Oct 22 15:44 ../../data/facebook-maria/866250103534243_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 230K Oct 25 17:50 ../../data/facebook-maria/486819048360070_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 217K Oct 25 17:50 ../../data/facebook-maria/1306772706118778_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 200K Oct 25 17:50 ../../data/facebook-maria/1909452959306518_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 184K Oct 25 17:50 ../../data/facebook-maria/118787682128186_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 170K Oct 26 17:56 ../../data/facebook-maria/1988415758095382_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 168K Oct 26 17:53 ../../data/facebook-maria/1979604895658060_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 161K Oct 25 17:49 ../../data/facebook-maria/142680509678003_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 161K Oct 26 17:56 ../../data/facebook-maria/142680509678003_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 160K Oct 25 17:50 ../../data/facebook-maria/1988415758095382_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 153K Oct 25 17:49 ../../data/facebook-maria/844368579056187_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 145K Oct 25 17:50 ../../data/facebook-maria/320205998443003_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 136K Oct 26 17:55 ../../data/facebook-maria/1724474021195839_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 135K Oct 25 17:49 ../../data/facebook-maria/1724474021195839_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 132K Oct 26 17:55 ../../data/facebook-maria/1909452959306518_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 130K Oct 25 17:49 ../../data/facebook-maria/185336475344755_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 128K Oct 26 17:52 ../../data/facebook-maria/132392547395104_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 124K Oct 26 17:54 ../../data/facebook-maria/486819048360070_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 116K Oct 26 17:56 ../../data/facebook-maria/1306772706118778_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 108K Oct 26 17:54 ../../data/facebook-maria/311739902631820_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 108K Oct 26 17:52 ../../data/facebook-maria/721270821393500_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 108K Oct 25 17:49 ../../data/facebook-maria/1674953605849716_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  95K Oct 25 17:49 ../../data/facebook-maria/505778186467207_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  84K Oct 25 17:49 ../../data/facebook-maria/119132632075224_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  82K Oct 26 17:54 ../../data/facebook-maria/119132632075224_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  76K Oct 26 17:51 ../../data/facebook-maria/228066954387757_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  72K Oct 25 17:49 ../../data/facebook-maria/889808494515226_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  70K Oct 25 17:49 ../../data/facebook-maria/128909594424200_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  70K Oct 25 17:49 ../../data/facebook-maria/1548641381825933_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  66K Oct 26 17:53 ../../data/facebook-maria/889808494515226_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  64K Oct 26 17:53 ../../data/facebook-maria/505778186467207_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  51K Oct 25 17:49 ../../data/facebook-maria/130010134396359_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  48K Oct 26 17:51 ../../data/facebook-maria/119087272129297_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  47K Oct 26 17:53 ../../data/facebook-maria/1548641381825933_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  43K Oct 25 17:49 ../../data/facebook-maria/351272391991842_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  40K Oct 26 17:51 ../../data/facebook-maria/119987445374151_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  40K Oct 25 17:49 ../../data/facebook-maria/1103661653103604_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  39K Oct 25 17:49 ../../data/facebook-maria/782660231920150_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  36K Oct 26 17:52 ../../data/facebook-maria/239708553223790_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  35K Oct 25 17:49 ../../data/facebook-maria/239708553223790_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  35K Oct 25 17:49 ../../data/facebook-maria/803659183149847_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  34K Oct 26 17:52 ../../data/facebook-maria/1961676614046008_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  34K Oct 25 17:49 ../../data/facebook-maria/1732562527048857_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  32K Oct 26 17:52 ../../data/facebook-maria/351272391991842_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  32K Oct 25 17:49 ../../data/facebook-maria/1453581968094172_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  29K Oct 26 17:52 ../../data/facebook-maria/130010134396359_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  27K Oct 25 17:49 ../../data/facebook-maria/1961676614046008_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  25K Oct 26 17:52 ../../data/facebook-maria/1453581968094172_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  25K Oct 26 17:51 ../../data/facebook-maria/782660231920150_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  24K Oct 26 17:55 ../../data/facebook-maria/1674953605849716_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  21K Oct 25 17:49 ../../data/facebook-maria/130583424251881_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  20K Oct 25 17:49 ../../data/facebook-maria/117224992282301_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  19K Oct 26 17:51 ../../data/facebook-maria/352124068563860_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  18K Oct 25 17:49 ../../data/facebook-maria/352124068563860_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  17K Oct 25 17:49 ../../data/facebook-maria/727610640755618_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  17K Oct 25 17:49 ../../data/facebook-maria/119987445374151_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  16K Oct 26 17:51 ../../data/facebook-maria/117529462276443_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  14K Oct 26 17:52 ../../data/facebook-maria/1103661653103604_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  13K Oct 26 17:51 ../../data/facebook-maria/130583424251881_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  11K Oct 25 17:49 ../../data/facebook-maria/1945917302354700_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  11K Oct 26 17:54 ../../data/facebook-maria/128909594424200_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  11K Oct 25 17:49 ../../data/facebook-maria/210130182858104_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 9.5K Oct 26 17:51 ../../data/facebook-maria/986662628142079_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 9.3K Oct 25 17:49 ../../data/facebook-maria/986662628142079_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 8.0K Oct 26 17:51 ../../data/facebook-maria/1773209126315380_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 8.0K Oct 26 17:51 ../../data/facebook-maria/727610640755618_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 7.5K Oct 26 17:51 ../../data/facebook-maria/1945917302354700_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 6.9K Oct 25 11:25 ../../data/facebook-maria/274834366346071_1505865600_1505952000_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 6.9K Oct 25 11:27 ../../data/facebook-maria/274834366346071_2017-09-20 00:00:00_2017-09-21 00:00:00_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 6.9K Oct 24 19:48 ../../data/facebook-maria/274834366346071_2017-09-20_2017-09-21_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 6.7K Oct 26 17:51 ../../data/facebook-maria/117224992282301_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 6.7K Oct 25 17:49 ../../data/facebook-maria/1201507393289356_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 6.6K Oct 25 17:49 ../../data/facebook-maria/1390525734396603_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 6.0K Oct 25 17:49 ../../data/facebook-maria/301495306993376_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 5.9K Oct 25 17:49 ../../data/facebook-maria/117529462276443_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 5.3K Oct 26 17:51 ../../data/facebook-maria/871880559633183_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 5.0K Oct 26 17:51 ../../data/facebook-maria/301495306993376_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-rw-r-- 1 istewart6 gtperson 4.5K Oct 25 13:48 ../../data/facebook-maria/PR_municipalities.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 4.2K Oct 25 14:36 ../../data/facebook-maria/location_group_data.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 4.0K Oct 26 17:51 ../../data/facebook-maria/803659183149847_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 4.0K Oct 25 17:49 ../../data/facebook-maria/1106549189385804_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 3.9K Oct 25 17:49 ../../data/facebook-maria/1513874938700905_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 3.9K Oct 26 17:51 ../../data/facebook-maria/105121436871597_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 3.7K Oct 26 17:51 ../../data/facebook-maria/477086339329141_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 3.7K Oct 25 17:49 ../../data/facebook-maria/477086339329141_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 3.6K Oct 26 17:51 ../../data/facebook-maria/210130182858104_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 3.5K Oct 25 17:49 ../../data/facebook-maria/871880559633183_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 3.0K Oct 25 17:49 ../../data/facebook-maria/105121436871597_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 1.8K Oct 25 17:49 ../../data/facebook-maria/247192592473869_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 1.8K Oct 26 17:51 ../../data/facebook-maria/1201507393289356_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 1.7K Oct 26 17:51 ../../data/facebook-maria/1106549189385804_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson 1.7K Oct 26 14:17 ../../data/facebook-maria/facebook_group_location_data.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  904 Oct 25 17:49 ../../data/facebook-maria/173314876566851_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  678 Oct 25 17:49 ../../data/facebook-maria/1673008566050997_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  302 Oct 25 17:49 ../../data/facebook-maria/1947578168849393_2017-09-20_2017-10-20_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  200 Oct 25 11:34 ../../data/facebook-maria/274834366346071_2017-09-20 00:00:00_2017-09-20 12:00:00_facebook_posts.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  166 Oct 26 17:51 ../../data/facebook-maria/1513874938700905_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  166 Oct 26 17:51 ../../data/facebook-maria/1673008566050997_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  166 Oct 22 15:10 ../../data/facebook-maria/171400810082867_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  166 Oct 26 17:51 ../../data/facebook-maria/173314876566851_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  166 Oct 26 17:51 ../../data/facebook-maria/1947578168849393_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  166 Oct 26 17:51 ../../data/facebook-maria/247192592473869_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson  166 Oct 22 15:10 ../../data/facebook-maria/866250103534243_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson    0 Oct 26 17:50 ../../data/facebook-maria/1390525734396603_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson    0 Oct 26 17:50 ../../data/facebook-maria/1732562527048857_2017-09-20_2017-10-20_facebook_comments.tsv\r\n",
      "-rw-r--r-- 1 istewart6 gtperson    0 Oct 26 17:50 ../../data/facebook-maria/185336475344755_2017-09-20_2017-10-20_facebook_comments.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!ls -haltS ../../data/facebook-maria/*.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5626 statuses to test\n"
     ]
    }
   ],
   "source": [
    "test_facebook_file = '../../data/facebook-maria/1773209126315380_2017-09-20_2017-10-20_facebook_posts.tsv'\n",
    "test_facebook_df = pd.read_csv(test_facebook_file, sep='\\t', index_col=0)\n",
    "print('%d statuses to test'%(test_facebook_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a sample of 500 statuses and mark the entities (if any)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*PORFAVOR*** Muy PREOCUPADA***por mi Querida Tia &Tio- Carlos (Jun) Vega Y Rosa Vega- Estancias el Vijia en Guayama . Rosanna Rivera de Orlando- sabe algo porfavor contact me.\n",
      "Buenas noches estoy tratando de obtener información de My familia en olimpo de la calle 6, Carmen Iris Suren Tirado Andrea Arroyo Arelis Arroyo de la calle 8, Jenniferlee Sierra Soto de la puente, si alguien tiene alguna información no importa la hora por favor envíeme un mensaje gracias\n",
      "Hola me gustaría saber si alguien conoce a Yaxie Vázquez ella vive en Guayama en el residencial Villamar, no hemos podido comunicarnos y estamos preocupados si alguien sabe de ella o su familia por favor déjenme saber por este medio. Gracias de antemano mano.\n",
      "I finally spoke to my Grandparents In Guayama from Carite they are ok. Thank God but they say Everyone up there lost alot but they are all ok.The Service for Claro is working at the moment so try calling your family members\n",
      "Ya supe que uno de mis hermanos y sobrinos estan bien pero alguien sabe si mi mama wanda santiago mi hermano Juank Ortiz y Emmanuel Ortiz Yamil Zoel Ramon Gonzalez Marta Mendez estan bien de la urb la hacienda calle 54 por favor se los agradecere en el alma llevo 3 dias sin dormir mano\n",
      "buenos días estoy todavía preocupado por mi familia ketty Acosta cruz y Sylvia Cruz por favor si alguien sabe cualquier información nada ayudará a que son de Guayma villa Rosa calle 2 Lizette Santiago Awilda Cruz Cynthia Cruz\n",
      "Mi amiga está buscando as sus padres . Si alguien tiene información de ellos por favor llama Cynthia a (770) 771-2272. Se llaman Nelson y Luisa Caro de Guayama URB. Hacienda AS 19 Calle 46.\n",
      "En la página de att.com explican: Sólo si tu pariente o amistades tienen ATT celular, puedes registrarte en la página del huracán María para que te notifiquen cuando al celular le llegue la seńal.\n",
      "Espero les sea útil la información, Dios les bendiga buenas noches❤️🇵🇷\n",
      "Hola buenas noches kiero saver de el barrio mosquito tengo mi mama y mi papa y mia DOS hija Dios mio pom tu mano💔💔💔💔💔😭😭😭😵\n"
     ]
    }
   ],
   "source": [
    "pd.np.random.seed(123)\n",
    "sample_size = 500\n",
    "status_txt = test_facebook_df.loc[:, 'status_message'].dropna(inplace=False).apply(str).replace('\\t', ' ').replace('\\n', ' ')\n",
    "status_sample = pd.np.random.choice(status_txt, size=sample_size, replace=False)\n",
    "print('\\n'.join(status_sample[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file for separate annotation\n",
    "group_id = os.path.basename(test_facebook_file).split('_')[0]\n",
    "data_dir = os.path.dirname(test_facebook_file)\n",
    "sample_file = os.path.join(data_dir, '%s_post_sample_txt.txt'%(group_id))\n",
    "with codecs.open(sample_file, 'w', encoding='utf-8') as sample_file_out:\n",
    "    for l in status_sample:\n",
    "        l_str = '%s\\n'%(l).decode('utf-8')\n",
    "        sample_file_out.write(l_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for sanity we only label the first 10 entities in each status because some of these statuses are TOO LONG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250 annotated lines later... (also includes empty lines)\n",
    "import re\n",
    "annotate_matcher = re.compile('(?<=<<)[\\w\\s\\,\\.]+(?=>>)')\n",
    "sample_annotated_file = os.path.join(data_dir, '%s_post_sample_annotated.txt'%(group_id))\n",
    "final_line = 249\n",
    "sample_annotated_txt = [l.strip() for i, l in enumerate(codecs.open(sample_annotated_file)) if i < final_line]\n",
    "sample_annotated_txt_mentions = [annotate_matcher.findall(l) for l in sample_annotated_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Guayama', 41), ('guayama', 16), ('Puerto Rico', 12), ('PR', 10), ('San Juan', 8), ('olimpo', 4), ('Hato Rey', 4), ('Arroyo', 4), ('Patillas', 4), ('Estados Unidos', 4)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "annotation_counts = Counter(reduce(lambda x,y: x+y, sample_annotated_txt_mentions))\n",
    "top_k = 10\n",
    "print(annotation_counts.most_common(top_k))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the most common entities annotated are the municipality of the group (`Guayama`) and the country (`Puerto Rico`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 lines\n",
      "processed 1000000 lines\n",
      "processed 2000000 lines\n",
      "processed 3000000 lines\n",
      "processed 4000000 lines\n",
      "processed 5000000 lines\n",
      "processed 6000000 lines\n",
      "processed 7000000 lines\n",
      "processed 8000000 lines\n",
      "processed 9000000 lines\n",
      "processed 10000000 lines\n",
      "processed 11000000 lines\n",
      "processed 12000000 lines\n",
      "processed 13000000 lines\n",
      "processed 14000000 lines\n",
      "processed 15000000 lines\n",
      "processed 16000000 lines\n",
      "processed 17000000 lines\n",
      "processed 18000000 lines\n",
      "processed 19000000 lines\n",
      "processed 20000000 lines\n",
      "processed 21000000 lines\n",
      "processed 22000000 lines\n",
      "processed 23000000 lines\n",
      "processed 24000000 lines\n",
      "processed 25000000 lines\n",
      "processed 26000000 lines\n",
      "processed 27000000 lines\n",
      "processed 28000000 lines\n",
      "processed 29000000 lines\n",
      "processed 30000000 lines\n",
      "processed 31000000 lines\n",
      "processed 32000000 lines\n",
      "processed 33000000 lines\n",
      "processed 34000000 lines\n",
      "processed 35000000 lines\n",
      "processed 36000000 lines\n",
      "processed 37000000 lines\n",
      "processed 38000000 lines\n",
      "processed 39000000 lines\n",
      "processed 40000000 lines\n",
      "processed 41000000 lines\n",
      "processed 42000000 lines\n",
      "processed 43000000 lines\n",
      "processed 44000000 lines\n",
      "processed 45000000 lines\n",
      "processed 46000000 lines\n",
      "processed 47000000 lines\n",
      "processed 48000000 lines\n",
      "processed 49000000 lines\n",
      "processed 50000000 lines\n",
      "processed 51000000 lines\n",
      "processed 52000000 lines\n",
      "processed 53000000 lines\n",
      "processed 54000000 lines\n",
      "processed 55000000 lines\n",
      "processed 56000000 lines\n",
      "processed 57000000 lines\n",
      "processed 58000000 lines\n",
      "processed 59000000 lines\n",
      "processed 60000000 lines\n",
      "processed 61000000 lines\n",
      "processed 62000000 lines\n",
      "processed 63000000 lines\n",
      "processed 64000000 lines\n",
      "processed 65000000 lines\n",
      "processed 66000000 lines\n",
      "processed 67000000 lines\n",
      "processed 68000000 lines\n",
      "processed 69000000 lines\n",
      "processed 70000000 lines\n",
      "processed 71000000 lines\n",
      "processed 72000000 lines\n",
      "processed 73000000 lines\n",
      "processed 74000000 lines\n",
      "processed 75000000 lines\n",
      "processed 76000000 lines\n",
      "processed 77000000 lines\n",
      "processed 78000000 lines\n",
      "processed 79000000 lines\n",
      "processed 80000000 lines\n",
      "processed 81000000 lines\n",
      "processed 82000000 lines\n",
      "processed 83000000 lines\n",
      "processed 84000000 lines\n",
      "processed 85000000 lines\n",
      "processed 86000000 lines\n",
      "processed 87000000 lines\n",
      "processed 88000000 lines\n",
      "processed 89000000 lines\n",
      "processed 90000000 lines\n",
      "processed 91000000 lines\n",
      "processed 92000000 lines\n",
      "processed 93000000 lines\n",
      "processed 94000000 lines\n",
      "processed 95000000 lines\n",
      "processed 96000000 lines\n",
      "processed 97000000 lines\n",
      "processed 98000000 lines\n",
      "processed 99000000 lines\n",
      "processed 100000000 lines\n",
      "processed 101000000 lines\n",
      "processed 102000000 lines\n",
      "processed 103000000 lines\n",
      "processed 104000000 lines\n",
      "processed 105000000 lines\n",
      "processed 106000000 lines\n",
      "processed 107000000 lines\n",
      "processed 108000000 lines\n",
      "processed 109000000 lines\n",
      "processed 110000000 lines\n",
      "processed 111000000 lines\n",
      "processed 112000000 lines\n",
      "processed 113000000 lines\n",
      "processed 114000000 lines\n",
      "processed 115000000 lines\n",
      "processed 116000000 lines\n",
      "processed 117000000 lines\n",
      "processed 118000000 lines\n",
      "processed 119000000 lines\n",
      "processed 120000000 lines\n",
      "processed 121000000 lines\n",
      "processed 122000000 lines\n",
      "processed 123000000 lines\n",
      "processed 124000000 lines\n",
      "processed 125000000 lines\n",
      "processed 126000000 lines\n",
      "processed 127000000 lines\n",
      "processed 128000000 lines\n",
      "processed 129000000 lines\n",
      "processed 130000000 lines\n",
      "processed 131000000 lines\n",
      "processed 132000000 lines\n",
      "processed 133000000 lines\n",
      "processed 134000000 lines\n",
      "processed 135000000 lines\n",
      "processed 136000000 lines\n",
      "processed 137000000 lines\n",
      "processed 138000000 lines\n",
      "processed 139000000 lines\n",
      "processed 140000000 lines\n",
      "processed 141000000 lines\n",
      "processed 142000000 lines\n",
      "processed 143000000 lines\n",
      "processed 144000000 lines\n",
      "processed 145000000 lines\n",
      "processed 146000000 lines\n",
      "processed 147000000 lines\n",
      "processed 148000000 lines\n",
      "processed 149000000 lines\n",
      "processed 150000000 lines\n",
      "processed 151000000 lines\n",
      "processed 152000000 lines\n",
      "processed 153000000 lines\n",
      "processed 154000000 lines\n",
      "processed 155000000 lines\n",
      "processed 156000000 lines\n",
      "processed 157000000 lines\n",
      "processed 158000000 lines\n",
      "processed 159000000 lines\n",
      "processed 160000000 lines\n",
      "processed 161000000 lines\n",
      "processed 162000000 lines\n",
      "processed 163000000 lines\n",
      "processed 164000000 lines\n",
      "processed 165000000 lines\n"
     ]
    }
   ],
   "source": [
    "import data_helpers\n",
    "reload(data_helpers)\n",
    "from data_helpers import get_mention_entity_lists\n",
    "mention_entity_lists = get_mention_entity_lists(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77000718\n"
     ]
    }
   ],
   "source": [
    "print(len(mention_entity_lists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1000000 mention strings\n",
      "processed 2000000 mention strings\n",
      "processed 3000000 mention strings\n",
      "processed 4000000 mention strings\n",
      "processed 5000000 mention strings\n",
      "processed 6000000 mention strings\n",
      "processed 7000000 mention strings\n",
      "processed 8000000 mention strings\n",
      "processed 9000000 mention strings\n",
      "processed 10000000 mention strings\n",
      "processed 11000000 mention strings\n",
      "processed 12000000 mention strings\n",
      "processed 13000000 mention strings\n",
      "processed 14000000 mention strings\n",
      "processed 15000000 mention strings\n",
      "processed 16000000 mention strings\n",
      "processed 17000000 mention strings\n",
      "processed 18000000 mention strings\n",
      "processed 19000000 mention strings\n",
      "processed 20000000 mention strings\n",
      "processed 21000000 mention strings\n",
      "processed 22000000 mention strings\n",
      "processed 23000000 mention strings\n",
      "processed 24000000 mention strings\n",
      "processed 25000000 mention strings\n",
      "processed 26000000 mention strings\n",
      "processed 27000000 mention strings\n",
      "processed 28000000 mention strings\n",
      "processed 29000000 mention strings\n",
      "processed 30000000 mention strings\n",
      "processed 31000000 mention strings\n",
      "processed 32000000 mention strings\n",
      "processed 33000000 mention strings\n",
      "processed 34000000 mention strings\n",
      "processed 35000000 mention strings\n",
      "processed 36000000 mention strings\n",
      "processed 37000000 mention strings\n",
      "processed 38000000 mention strings\n",
      "processed 39000000 mention strings\n",
      "processed 40000000 mention strings\n",
      "processed 41000000 mention strings\n",
      "processed 42000000 mention strings\n",
      "processed 43000000 mention strings\n",
      "processed 44000000 mention strings\n",
      "processed 45000000 mention strings\n",
      "processed 46000000 mention strings\n",
      "processed 47000000 mention strings\n",
      "processed 48000000 mention strings\n",
      "processed 49000000 mention strings\n",
      "processed 50000000 mention strings\n",
      "processed 51000000 mention strings\n",
      "processed 52000000 mention strings\n",
      "processed 53000000 mention strings\n",
      "processed 54000000 mention strings\n",
      "processed 55000000 mention strings\n",
      "processed 56000000 mention strings\n",
      "processed 57000000 mention strings\n",
      "processed 58000000 mention strings\n",
      "processed 59000000 mention strings\n",
      "processed 60000000 mention strings\n",
      "processed 61000000 mention strings\n",
      "processed 62000000 mention strings\n",
      "processed 63000000 mention strings\n",
      "processed 64000000 mention strings\n",
      "processed 65000000 mention strings\n",
      "processed 66000000 mention strings\n",
      "processed 67000000 mention strings\n",
      "processed 68000000 mention strings\n",
      "processed 69000000 mention strings\n",
      "processed 70000000 mention strings\n",
      "processed 71000000 mention strings\n",
      "processed 72000000 mention strings\n",
      "processed 73000000 mention strings\n",
      "processed 74000000 mention strings\n",
      "processed 75000000 mention strings\n",
      "processed 76000000 mention strings\n",
      "processed 77000000 mention strings\n"
     ]
    }
   ],
   "source": [
    "# need inverse to get entity-mention and then restrict to location entities\n",
    "from collections import defaultdict\n",
    "entity_mention_lists = defaultdict(list)\n",
    "ctr = 0\n",
    "for m, e_list in mention_entity_lists.iteritems():\n",
    "    for e in e_list:\n",
    "        entity_mention_lists[e].append(m)\n",
    "    ctr += 1\n",
    "    if(ctr % 1000000 == 0):\n",
    "        print('processed %d mention strings'%(ctr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get location strings\n",
    "import gzip\n",
    "location_entities = [l.strip() for l in gzip.open('/hg190/corpora/crosswikis-data.tar.bz2/location_urls.gz', 'r')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_mention_lists = {l : entity_mention_lists[l] for l in location_entities}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for all locations without mentinos, add the entity name as default\n",
    "for loc, mentions in location_mention_lists.iteritems():\n",
    "    if(len(mentions) == 0):\n",
    "        new_mentions = [loc.replace('_', ' ')]\n",
    "        location_mention_lists[loc] = new_mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 mention lists\n",
      "processed 100000 mention lists\n",
      "processed 200000 mention lists\n",
      "processed 300000 mention lists\n",
      "processed 400000 mention lists\n",
      "processed 500000 mention lists\n",
      "processed 600000 mention lists\n",
      "processed 700000 mention lists\n",
      "processed 800000 mention lists\n",
      "10721949 location mentions\n"
     ]
    }
   ],
   "source": [
    "# now combine all the mentions\n",
    "location_mention_lexicon = set()\n",
    "for i, v in enumerate(location_mention_lists.values()):\n",
    "    location_mention_lexicon.update(v) \n",
    "    if(i % 100000 == 0):\n",
    "        print('processed %d mention lists'%(i))\n",
    "print('%d location mentions'%(len(location_mention_lexicon)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to file\n",
    "location_mention_lexicon_out_file = '/hg190/corpora/crosswikis-data.tar.bz2/location_mention_lexicon.gz'\n",
    "with gzip.open(location_mention_lexicon_out_file, 'w') as out_file:\n",
    "    for l in sorted(location_mention_lexicon):\n",
    "        out_file.write('%s\\n'%(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test recall\n",
    "We test the ability of (1) NER and (2) location mention lexicon to extract the mention candidates.\n",
    "\n",
    "First we need to get tags for both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get NER tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hg190/istewart6/crisis_language/lib/python2.7/site-packages/nltk/tag/stanford.py:183: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.tag.corenlp.CoreNLPPOSTagger\u001b[0m or \u001b[91mnltk.tag.corenlp.CoreNLPNERTagger\u001b[0m instead.\n",
      "  super(StanfordNERTagger, self).__init__(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag.stanford import StanfordNERTagger\n",
    "stanford_dir = 'stanford-ner-2017-06-09'\n",
    "jar_file = os.path.join(stanford_dir, 'stanford-ner.jar')\n",
    "model_file = os.path.join(stanford_dir, 'classifiers/spanish.ancora.distsim.s512.crf.ser.gz')\n",
    "tagger = StanfordNERTagger(model_filename=model_file, path_to_jar=jar_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "tag_txt = lambda x: tagger.tag(tokenizer.tokenize(x))\n",
    "status_sample_ner_tagged = map(tag_txt, status_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix format\n",
    "status_sample_ner_tagged = map(lambda x: ' '.join(['%s/%s'%(y) for y in x]), status_sample_ner_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*/O PORFAVOR/ORG */O */O */O Muy/O PREOCUPADA/O */O */O */O por/O mi/O Querida/ORG Tia/ORG &/ORG Tio/ORG -/ORG Carlos/ORG (/ORG Jun/ORG )/ORG Vega/ORG Y/O Rosa/OTROS Vega/OTROS -/O Estancias/O el/O Vijia/ORG en/O Guayama/LUG ./O Rosanna/PERS Rivera/PERS de/O Orlando/OTROS -/O sabe/O algo/O porfavor/O contact/O me/O ./O\n",
      "Buenas/O noches/O estoy/O tratando/O de/O obtener/O información/O de/O My/O familia/O en/O olimpo/O de/O la/O calle/O 6/O ,/O Carmen/PERS Iris/PERS Suren/PERS Tirado/O Andrea/PERS Arroyo/PERS Arelis/PERS Arroyo/PERS de/O la/O calle/O 8/O ,/O Jenniferlee/PERS Sierra/PERS Soto/PERS de/O la/O puente/O ,/O si/O alguien/O tiene/O alguna/O información/O no/O importa/O la/O hora/O por/O favor/O envíeme/O un/O mensaje/O gracias/O\n",
      "Hola/O me/O gustaría/O saber/O si/O alguien/O conoce/O a/O Yaxie/PERS Vázquez/PERS ella/O vive/O en/O Guayama/LUG en/O el/O residencial/O Villamar/PERS ,/O no/O hemos/O podido/O comunicarnos/O y/O estamos/O preocupados/O si/O alguien/O sabe/O de/O ella/O o/O su/O familia/O por/O favor/O déjenme/O saber/O por/O este/O medio/O ./O Gracias/O de/O antemano/OTROS mano/O ./O\n",
      "I/O finally/O spoke/O to/O my/O Grandparents/ORG In/ORG Guayama/ORG from/O Carite/PERS they/O are/O ok/O ./O Thank/ORG God/ORG but/ORG they/ORG say/ORG Everyone/ORG up/ORG there/ORG lost/ORG alot/ORG but/ORG they/O are/O all/ORG ok.The/ORG Service/ORG for/ORG Claro/ORG is/ORG working/ORG at/ORG the/ORG moment/ORG so/O try/O calling/O your/O family/O members/O\n",
      "Ya/O supe/O que/O uno/O de/O mis/O hermanos/O y/O sobrinos/O estan/O bien/O pero/O alguien/O sabe/O si/O mi/O mama/O wanda/O santiago/O mi/O hermano/O Juank/PERS Ortiz/PERS y/O Emmanuel/PERS Ortiz/PERS Yamil/PERS Zoel/PERS Ramon/PERS Gonzalez/PERS Marta/PERS Mendez/PERS estan/O bien/O de/O la/O urb/O la/O hacienda/O calle/O 54/O por/O favor/O se/O los/O agradecere/O en/O el/O alma/O llevo/O 3/O dias/O sin/O dormir/O mano/O\n",
      "buenos/O días/O estoy/O todavía/O preocupado/O por/O mi/O familia/O ketty/PERS Acosta/PERS cruz/O y/O Sylvia/PERS Cruz/PERS por/O favor/O si/O alguien/O sabe/O cualquier/O información/O nada/O ayudará/O a/O que/O son/O de/O Guayma/LUG villa/LUG Rosa/LUG calle/O 2/O Lizette/PERS Santiago/PERS Awilda/PERS Cruz/PERS Cynthia/PERS Cruz/PERS\n",
      "Mi/O amiga/O está/O buscando/O as/O sus/O padres/O ./O Si/O alguien/O tiene/O información/O de/O ellos/O por/O favor/O llama/O Cynthia/PERS a/O (770)/O 771-2272/O ./O Se/O llaman/O Nelson/PERS y/O Luisa/PERS Caro/PERS de/O Guayama/ORG URB/ORG ./O Hacienda/O AS/ORG 19/O Calle/O 46/O\n",
      "En/O la/O página/O de/O att.com/ORG explican/O :/O Sólo/O si/O tu/O pariente/O o/O amistades/O tienen/O ATT/ORG celular/O ,/O puedes/O registrarte/O en/O la/O página/O del/O huracán/O María/PERS para/O que/O te/O notifiquen/O cuando/O al/O celular/O le/O llegue/O la/O seńal/O ./O\n",
      "Espero/O les/O sea/O útil/O la/O información/O ,/O Dios/PERS les/O bendiga/O buenas/O noches/O ❤/O ️/O �/O �/O �/O �/O\n",
      "Hola/O buenas/O noches/O kiero/ORG saver/ORG de/O el/O barrio/O mosquito/O tengo/O mi/O mama/O y/O mi/O papa/O y/O mia/O DOS/ORG hija/ORG Dios/ORG mio/ORG pom/ORG tu/O mano/O �/O �/O �/O �/O �/O �/O �/O �/O �/O �/O �/O �/O �/O �/O �/O �/O �/O �/O\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join(status_sample_ner_tagged[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Guayama', 84), ('Carr', 38), ('Dios', 30), ('Puerto_Rico', 27), ('PR', 24), ('Urb', 20), ('Gracias', 17), ('Maria', 12), ('San_Juan', 11), ('Olimpo', 9)]\n"
     ]
    }
   ],
   "source": [
    "from data_helpers import collect_entities_from_txt\n",
    "from collections import Counter\n",
    "status_sample_entity_list = map(collect_entities_from_txt, status_sample_ner_tagged)\n",
    "status_sample_entity_counts = Counter(reduce(lambda x,y: x+y, status_sample_entity_list))\n",
    "print(status_sample_entity_counts.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to file!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_out_file = os.path.join('../../data/facebook-maria/', '%s_post_sample_ner.txt'%(group_id))\n",
    "with codecs.open(ner_out_file, 'w', encoding='utf-8') as ner_out:\n",
    "    for l in status_sample_ner_tagged:\n",
    "        ner_out.write('%s\\n'%(l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get lexicon tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lexicon. We'll use all n-grams on range 1-4 and look for the overlap between the lexicon and n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload corpus because dead kernel\n",
    "import codecs\n",
    "status_sample_file_name = '../../data/facebook-maria/1773209126315380_post_sample_txt.txt'\n",
    "status_sample = [l.strip() for l in codecs.open(status_sample_file_name, 'r', encoding='utf-8')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "min_df = 1\n",
    "max_df = 0.3\n",
    "ngram_range = (1,4)\n",
    "tokenizer = TweetTokenizer()\n",
    "cv = CountVectorizer(min_df=1, max_df=max_df, ngram_range=ngram_range, tokenizer=tokenizer.tokenize)\n",
    "status_sample_dtm = cv.fit_transform(status_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 60601)\n"
     ]
    }
   ],
   "source": [
    "print(status_sample_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload location mention strings because dead kernel\n",
    "import gzip\n",
    "location_mention_lexicon = [l.strip() for l in gzip.open('/hg190/corpora/crosswikis-data.tar.bz2/location_mention_lexicon.gz')]\n",
    "location_entities = [l.strip().replace('_', ' ') for l in gzip.open('/hg190/corpora/crosswikis-data.tar.bz2/location_urls.gz', 'r')]\n",
    "location_mention_lexicon += location_entities\n",
    "location_mention_lexicon = sorted(set(location_mention_lexicon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2057 overlap words\n",
      "[u'615', u'624', u'65', u'694', u'696', u'700', u'75', u'77', u'78', u'80', u'80 %', u'81', u'824', u'829', u'830', u'833', u'84', u'842', u'843', u'845', u'852', u'88', u'90', u'91', u'938', u'95', u'950', u'97', u'975', u'99', u'999', u'9am', u':', u':p', u'<3', u'a casa', u'a couple', u'a ir', u'a la', u'a little', u'a message', u'a partir', u'a section', u'aa', u'aaa', u'abajo', u'able', u'about', u'about it', u'about them', u'above', u'abrir', u'across', u'action', u'ada', u'address', u'addresses', u'aeropuerto', u'aeropuerto de', u'aeropuerto internacional', u'aeropuertos', u'after', u'again', u'age', u'agency', u'ago', u'agua', u'aguadilla', u'aguadilla airport', u'aguas', u'aguas buenas', u'ah', u'aibonito', u'aircraft', u'aire', u'airlines', u'airport', u'aka', u'al', u'al parque', u'al pueblo', u'ala', u'alcalde', u'alejandrino', u'alerta', u'alex', u'algo', u'alguen', u'alguna', u'alguna otra', u'algunas', u'algunos', u'alida', u'alimentos', u'all', u'all the', u'alma', u'alot', u'already', u'already have']\n"
     ]
    }
   ],
   "source": [
    "# get lexicon overlap\n",
    "ivoc = {v:k for k,v in cv.vocabulary_.iteritems()}\n",
    "status_sample_lexicon = sorted(set(cv.vocabulary_.keys()) & set(location_mention_lexicon))\n",
    "print('%d overlap words'%(len(status_sample_lexicon)))\n",
    "print(status_sample_lexicon[100:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_sample_lexicon_idx = set([cv.vocabulary_[v] for v in status_sample_lexicon])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'vega', u'carlos', u'tio', u'el', u'orlando', u'tia', u'sabe', u'rivera', u'guayama', u'contact', u'rosa', u'querida', u'*', u'me', u'muy', u'estancias', u'algo'], [u'iris', u'puente', u'la calle', u'un', u'familia', u'la puente', u'hora', u'calle', u'sierra', u'no', u'favor', u'mensaje', u'de la calle', u'olimpo', u'alguna', u'gracias', u'soto', u'tiene', u'de la', u'my'], [u'villamar', u'este', u'me', u'guayama', u'hola', u'medio', u'ella o', u'familia', u'su', u'no', u'el', u'gracias', u'sabe', u'favor', u'ella'], [u'finally', u'to', u'calling', u'members', u'family', u'at', u'service', u'for', u'god', u'everyone', u'thank', u'they say', u'but', u'guayama', u'family members', u'at the moment', u'moment', u'so', u'are', u'there', u'are all', u'lost', u'in', u'is', u'ok', u'try', u'they', u'they are', u'from', u'at the', u'your', u'alot', u'say', u'up', u'working', u'my', u'up there', u'the', u'all'], [u'hacienda', u'alma', u'gonzalez', u'sabe', u'santiago', u'sin', u'54', u'wanda', u'se', u'pero', u'la hacienda', u'calle', u'marta', u'que', u'los', u'el', u'ya', u'bien', u'uno', u'de la', u'favor'], [u'villa rosa', u'cruz', u'buenos', u'familia', u'sabe', u'2', u'son', u'villa', u'que', u'calle', u'rosa', u'sylvia', u'nada', u'santiago', u'favor'], [u'hacienda', u'sus', u'nelson', u'se', u'guayama', u'as', u'19', u'amiga', u'46', u'buscando', u'ellos', u'calle', u'tiene', u'caro', u'favor'], [u'al', u'te', u'tu', u'celular', u'que', u':', u'le', u'tienen', u'del', u'para'], [u'les', u'sea'], [u'el', u'tu', u'mia', u'hola', u'barrio', u'dos', u'mosquito', u'mio']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "valid_indices = [set(row.nonzero()[1]) & set(status_sample_lexicon_idx) for row in status_sample_dtm]\n",
    "# convert to vocab\n",
    "status_sample_lexicon_entities = [[ivoc[i] for i in idx] for idx in valid_indices]\n",
    "print(status_sample_lexicon_entities[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test mention extraction scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reload NER tags because dead kernel\n",
    "import codecs\n",
    "from data_helpers import collect_entities_from_txt\n",
    "status_sample_ner_tagged = [l.strip() for l in codecs.open('../../data/facebook-maria/1773209126315380_post_sample_ner.txt')]\n",
    "# collect entities\n",
    "status_sample_ner_entity_list = map(collect_entities_from_txt, status_sample_ner_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from itertools import izip\n",
    "def test_precision_recall(tagged_entity_list, gold_entity_list):\n",
    "    \"\"\"\n",
    "    Compute the precision and recall of entity recognition \n",
    "    based on the number of entities recovered out of\n",
    "    all possible entities.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tagged_entity_list : list\n",
    "    gold_entity_list : list\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    all_false_tags : set\n",
    "    all_missed_tags : set\n",
    "    precision : float\n",
    "    recall : float\n",
    "    \"\"\"\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    all_false_tags = []\n",
    "    all_missed_tags = []\n",
    "    for tagged, gold in izip(tagged_entity_list, gold_entity_list):\n",
    "        true_tags = set(tagged) & set(gold)\n",
    "        false_tags = set(tagged) - set(gold)\n",
    "        missed_tags = set(gold) - set(tagged)\n",
    "        TP += len(true_tags)\n",
    "        FP += len(false_tags)\n",
    "        FN += len(missed_tags)\n",
    "        all_false_tags.append(list(false_tags))\n",
    "        all_missed_tags.append(list(missed_tags))\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    return all_false_tags, all_missed_tags, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load gold entity lists\n",
    "import re, os, codecs\n",
    "annotate_matcher = re.compile('(?<=<<)[\\w\\s\\,\\.]+(?=>>)')\n",
    "sample_annotated_file = '../../data/facebook-maria/1773209126315380_post_sample_annotated.txt'\n",
    "final_line = 249\n",
    "sample_annotated_txt = [l.strip() for i, l in enumerate(codecs.open(sample_annotated_file)) if i < final_line]\n",
    "sample_annotated_txt_mentions = [annotate_matcher.findall(l) for l in sample_annotated_txt]\n",
    "N = len(sample_annotated_txt_mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER precision = 0.106, recall = 0.201\n"
     ]
    }
   ],
   "source": [
    "ner_false_tags, ner_missed_tags, ner_precision, ner_recall = test_precision_recall(status_sample_ner_entity_list[:N], sample_annotated_txt_mentions)\n",
    "print('NER precision = %.3f, recall = %.3f'%(ner_precision, ner_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow! That recall is terrible. This might have to do with the noise of the data hurting recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicon precision = 0.008, recall = 0.074\n"
     ]
    }
   ],
   "source": [
    "_, _, lexicon_precision, lexicon_recall = test_precision_recall(status_sample_lexicon_entities[:N], sample_annotated_txt_mentions)\n",
    "print('lexicon precision = %.3f, recall = %.3f'%(lexicon_precision, lexicon_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision is bad, which makes sense because of all the common words that snuck into the lexicon. It also has worse recall, maybe because we didn't collect the mentions correctly or because we should have lowercased everything first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do that as a sanity check!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_mention_lower = map(str.lower, location_mention_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'vega', u'carlos', u'tio', u'el', u'orlando', u'rosanna', u'sabe', u'rivera', u'guayama', u'contact', u'rosa', u'querida', u'*', u'de orlando', u'tia', u'me', u'muy', u'estancias', u'jun', u'algo'], [u'un', u'familia', u'calle', u'soto', u'carmen', u'hora', u'arroyo', u'no', u'buenas', u'tiene', u'gracias', u'iris', u'buenas noches', u'sierra', u'mensaje', u'alguna', u'de la', u'favor', u'la calle', u'la puente', u'andrea', u'de la calle', u'puente', u'olimpo', u'my'], [u'villamar', u'este', u'me', u'guayama', u'hola', u'medio', u'ella o', u'familia', u'su', u'no', u'favor', u'mano', u'el', u'gracias', u'sabe', u'vive', u'ella'], [u'finally', u'to', u'calling', u'members', u'family', u'at', u'service', u'for', u'god', u'everyone', u'thank', u'they say', u'but', u'guayama', u'family members', u'at the moment', u'moment', u'so', u'are', u'there', u'are all', u'lost', u'in', u'is', u'ok', u'try', u'they', u'they are', u'from', u'at the', u'your', u'claro', u'alot', u'say', u'up', u'working', u'my', u'up there', u'the', u'all'], [u'ramon', u'el', u'se', u'54', u'calle', u'mama', u'bien', u'santiago', u'emmanuel', u'mendez', u'pero', u'supe', u'mano', u'sin', u'uno', u'ortiz', u'hacienda', u'alma', u'gonzalez', u'que', u'marta', u'mis', u'de la', u'favor', u'hermanos', u'sabe', u'dias', u'wanda', u'la hacienda', u'los', u'ya'], [u'villa rosa', u'cruz', u'buenos', u'cynthia', u'familia', u'sabe', u'2', u'son', u'villa', u'que', u'calle', u'rosa', u'de guayma', u'sylvia', u'nada', u'santiago', u'favor', u'acosta'], [u'llama', u'hacienda', u'sus', u'nelson', u'favor', u'se', u'guayama', u'as', u'19', u'cynthia', u'amiga', u'46', u'buscando', u'ellos', u'calle', u'luisa', u'tiene', u'caro', u'padres'], [u'al', u'te', u'en la', u'tu', u'att', u'celular', u'que', u':', u'le', u'tienen', u'del', u'para'], [u'les', u'sea', u'espero', u'buenas', u'buenas noches'], [u'el', u'mosquito', u'tu', u'papa', u'mia', u'hola', u'pom', u'el barrio', u'buenas', u'mano', u'mama', u'barrio', u'saver', u'dos', u'buenas noches', u'mio']]\n"
     ]
    }
   ],
   "source": [
    "min_df = 1\n",
    "max_df = 0.3\n",
    "ngram_range = (1,4)\n",
    "tokenizer = TweetTokenizer()\n",
    "cv = CountVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range, tokenizer=tokenizer.tokenize, lowercase=True)\n",
    "status_sample_dtm = cv.fit_transform(status_sample)\n",
    "ivoc = {v:k for k,v in cv.vocabulary_.iteritems()}\n",
    "status_sample_lexicon = sorted(set(cv.vocabulary_.keys()) & set(location_mention_lower))\n",
    "status_sample_lexicon_idx = set([cv.vocabulary_[v] for v in status_sample_lexicon])\n",
    "valid_indices = [set(row.nonzero()[1]) & set(status_sample_lexicon_idx) for row in status_sample_dtm]\n",
    "# convert to vocab\n",
    "status_sample_lexicon_entities_lower = [[ivoc[i] for i in idx] for idx in valid_indices]\n",
    "print(status_sample_lexicon_entities_lower[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lexicon precision = 0.037, recall = 0.421\n"
     ]
    }
   ],
   "source": [
    "sample_annotated_txt_mentions_lower = map(lambda x: map(str.lower, x), sample_annotated_txt_mentions)\n",
    "lexicon_false_tags, lexicon_missed_tags, lexicon_precision, lexicon_recall = test_precision_recall(status_sample_lexicon_entities_lower[:N], sample_annotated_txt_mentions_lower)\n",
    "print('lexicon precision = %.3f, recall = %.3f'%(lexicon_precision, lexicon_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better recall! I guess the lexicon isn't as garbage as I thought. \n",
    "\n",
    "What is the lexicon getting that NER is missing? Maybe we have to combine both for \n",
    "\n",
    "**MAXIMUM OVERDRIVE**\n",
    "\n",
    "but let's get an idea of the errors first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "228\n",
      "['19 calle', '65th infantry', 'adjuntas', 'aguirre', 'aguirre, pr', 'anasco', 'apartamento santa rosa', 'areciblo pueblo', 'atenas college', 'av. franklin delano roosevelt', 'ave barbosa', 'ave baxter', 'ave domenech', 'ave hostos', 'ave lomas verdes', 'ave los dominicos', 'ave los patriotas', 'ave magnolia', 'ave maruca', 'ave pedro albizu campos', 'ave ponce de leon', 'ave ramon l rodriguez', 'ave ramon luis rivera', 'ave roberto h todd', 'ave. de diego', 'ave. irlanda height', 'ave. los veteranos', 'ave. montecarlo', 'ave. santa juanita', 'bairoa', 'barceloneta', 'barreras', 'barriada santa ana', 'barrio blondet', 'barrio brandeis', 'barrio branderi', 'barrio corazon', 'barrio de carite', 'barrio del olvido', 'barrio linea capo', 'barrio olimpo', 'barrio pueblo', 'barrio singapur', 'barrios obrero', 'bayamon gardens', 'bda blondet', 'bda santa ana', 'blonde', 'blondet', 'blvd dr.', 'bo candelaria', 'bo. carite', 'bomberos de guayama', 'boqueron', 'cabo rojo', 'calimano', 'calle 2', 'calle 3', 'calle 4', 'calle 46', 'calle 54', 'calle 6', 'calle 615', 'calle a', 'calle alondra', 'calle aracibo', 'calle arzuaga', 'calle b', 'calle baldorioty', 'calle c', 'calle casia', 'calle dr. vidal', 'calle e', 'calle enrique gonzalez', 'calle francisco', 'calle girasol', 'calle hostos', 'calle igualdad', 'calle jazmin', 'calle juan calaf', 'calle las flores', 'calle luis munoz rivera', 'calle marginal', 'calle morales bo hato tejas', 'calle navarra', 'calle obispado', 'calle palestina', 'calle san claudio', 'calle san jorge', 'calle san millan', 'calle santa balbara', 'calle tartak', 'calle villa', 'camuy', 'caparra heights', 'carite', 'cariterguayama', 'carolina pueblo', 'carolina shoppin ctr', 'carr 122', 'carr 159', 'carr 2', 'carr estatal', 'carr. 829', 'carr. 845', 'casa paola', 'ceiba airport', 'centro de manejo de emergencias', 'christainsted', 'ciales', 'cidra', 'comercial bonsai plaza', 'comerio', 'comunidad villodas', 'corchado', 'cupey', 'dorado', 'east end', 'el barrio mosquito', 'emancipation gardens', 'esquina hostos', 'fernandez juncos', 'gallows bay', 'guayama', 'guayama pr', 'guayama urb', 'guayma', 'guaynabo', 'guyama', 'hacienda as', 'hosp de vets', 'hospital hima', 'hospital lafayette', 'hospital san lucas', 'isla grande', 'jardines de guamani', 'jobos guayama', 'kinghill', 'la calle 8', 'la carr 15', 'la gran parada', 'la isla del cordero', 'la plaza de guayama', 'la urb', 'lajas', 'lares', 'las mareas', 'liddi palo seco maunabo', 'los barros', 'luis pales matos', 'maricao', 'marina', 'max sanchez', 'mayaguez', 'minillas center', 'moca', 'n.  marshall st', 'naguabo', 'nemesio canales', 'nuevo amanecer', 'old san juan', 'palmar del rey', 'palmas bajas', 'paseo de colon', 'patillas marin', 'pensylvannia', 'plaza victoria', 'plazas las americas', 'ponce', 'ponce airport', 'ponce bypass', 'pto rico', 'pueblito del carmen', 'puerto rice', 'puerto rico', 'ramey', 'ranchos guayama', 'reparto la sabana', 'residencia carioca', 'residencial calimano', 'residencial villamar', 'rexville plaza', 'rincon', 'rio piedras', 'sabana grande', 'sabana hoyos', 'san antonio', 'san german', 'san juan airport', 'san juan amf', 'san sebastian', 'sector candelaria', 'st lancaster', 'sunny isle', 'tjig', 'tjps', 'tjrv', 'tjsj auto', 'tjvq', 'toa baja', 'trujillo alto', 'tutu mall', 'urb hacienda', 'urb la pradera', 'urb vives', 'urb. dorado', 'urb. jardines de fagot', 'urb. jardines de guamani', 'urb. la hacienda', 'urb. los algarrobos', 'urb. plaza encantada', 'urb. vives', 'urbanizacion la hacienda', 'utuado', 'valles de guayama', 'vega alta', 'vega baja', 'veterans annex', 'vieques', 'vieques airport', 'villa caribe', 'villa carolina', 'villa la pica', 'villa universitaria', 'villas caney', 'villodas', 'washington, dc', 'wingohoking st']\n"
     ]
    }
   ],
   "source": [
    "ner_missed_tags_list = reduce(lambda x,y: x+y, ner_missed_tags)\n",
    "lexicon_missed_tags_list = reduce(lambda x,y: x+y, lexicon_missed_tags)\n",
    "ner_missed_tags_set = set(map(str.lower, ner_missed_tags_list))\n",
    "lexicon_missed_tags_set = set(lexicon_missed_tags_list)\n",
    "joint_missed_tags = ner_missed_tags_set & lexicon_missed_tags_set\n",
    "print(len(joint_missed_tags))\n",
    "print(sorted(joint_missed_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! So both NER and the lexicon missed things like:\n",
    "\n",
    "- obscure neighborhoods (`barrio blondet`)\n",
    "- roads (`calle 46`, `ave. montecarlo`)\n",
    "- buildings (`apartamento santa rosa`)\n",
    "- long names (`aguirre, pr`)\n",
    "- respellings (`pto rico`)\n",
    "\n",
    "Why are obvious matches like `guayama`, `san antonio` are not getting caught when they are actually in the lexicon!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mention string \"guayama\" not detected in the following statuses\n",
      "[u'palmas bajas-guayama video gracias a la dama r ivelisse ortiz']\n",
      "test mention string \"san antonio\" not detected in the following statuses\n",
      "[u'last updated oct. 1, 2017, 7 p.m. etoct. 1, 2017, 7 p.m. ethurricane mariacaribbeanmonday will see more mail operations across puerto rico and the virgin islands as inbound mail from the mainland continues to be transported and processed across the caribbean.   postal employees are in full force providing mail and postal retail services for customers in postal facilities that were not significantly damaged by hurricane maria.if an office is damaged, postal officials are designating nearby locations for customers to receive mail. in some cases in both puerto rice and the virgin islands, letter carriers have begun delivery where safe.puerto ricopartial delivery has started today where safe to do so for the following locations:65th infantry, station, 100 calle alondra, 00923.bayamon, branch, 100 ave ramon l rodriguez, 00956.cupey, station, 369 calle san claudio frnt, 00926.hato rey, station, 361 calle juan calaf, 00917.old san juan, station, 100 paseo de colon ste 1, 00901.rio piedras, station, 112 calle arzuaga ste 1, 00928.vieques, main po, 97 calle luis munoz rivera ste 103, 00765.customers served by the following offices can pick up their mail at the alternate locations listed.office      alternate office   aibonito, 00705.    cayey, 00736.     areciblo pueblo, 00612.    arecibo mpo, 00613.   arroyo, 00714.      patillas, 00723.   bayamon gardens, 00957.  bayamon branch, 00956.   canovanas, 00729.    rio grande, 00745.   caparra heights, 00968.    hato rey, 00917.   cotto, 00612.      arecibo mpo, 00613.   fernandez juncos, 00909.  santurce station, 00907. guayama, 00784.    salinas, 00751.     juncos, 00777.      gurabo, 00778.     la plata, 00786.    cayey, 00736.     luquillo, 00773.    fajardo, 00738.   plaza carolina, 00985.    carolina mpo, 00983.   plazas las americas, 00918.  san juan gpo, 00953.   punta santiago, 00741.    humacoa, 00741.   santa isabel, 00757.    coamo, 00769.     trujillo alto, 00976.    65th infantry, 00924    the following offices will be open for retail, po box mail and package pickup, starting at 9 a.m.   regular closing hours will be observed.adjuntas, 00601.aguirre, 00704.anasco, 00610.arecibo, 00612.barceloneta, 00617.boqueron, 00622.cabo rojo, 00623.camuy, 00627.carolina, 00979.carolina pueblo, 00986.catano, 00962.cayey, 00736.ciales, 00638.coamo, 00762.comerio, 00782.cidra, 00739.dorado, 00646.guaynabo, 00966.lajas, 00667.lares, 00669.maricao, 00606.marina, 00680.mayaguez, 00680.minillas center, 00940.moca, 00676.naguabo, 00718.north, 00726.patillas, 00723.ponce, 00716.ramey, 00604.rincon, 00677.sabana grande, 00637.sabana hoyos, 00688.san antonio, 00690.san german, 00683.san juan, 00933.san sebastian, 00685.santurce, 00908.toa baja, 00949.utuado, 00641.vega alta, 00692.vega baja, 00693victoria, 00605.san juan airport retail, 00937.the following offices are open po box mail and package pickup, starting at 9 a.m. regular closing hours will be observed. aguas buenas, 00703. san juan amf, 00985. ceiba, 00735. corozal, 00783. gurabo, 00778. naranjito, 00719. orocovis, 00720. barrios obrero, 00915.the san juan p&dc and catano dmdu annex are closed and not accepting fast appointments. st. thomasretail services and mail and package pickup tomorrow, sunday, oct. 1, will occur during regular sunday hours. offices open for mail and package pickup remain the same.east end /tutu mall, 00805. charlotte amalie, mail handout service, 00801, 00803, 00805. street addresses, 00802.customers served by the following offices can pick up their mail at the alternate locations.regular office  alternate   veterans annex, 00802.  charlotte amalie, 00801.emancipation gardens, 00802.charlotte amalie, 00801. st. johncustomers service by the following officecan pick up their mail at the alternate location.regular    alternate      cruz bay, 00830.  trucks in front of building st. croixthe following offices will be open for retail, po box mail and package pickup. regular sunday hours will be observed.christainsted, 00820. downtown, 00820.gallows bay, 00824. sunny isle, 00823.   customers service by the following office can pick up their mail at the alternate location.regular  alternate   kinghill, 00840.  frederiksted, 00850.               usps is not accepting priority mail express service or shipment of live animals for the following zip codes: 006, 007, 008 and 009. the following island is closed for retail services, package or mail pickup.culebra, 00775.']\n"
     ]
    }
   ],
   "source": [
    "test_mention_strings = ['guayama', 'san antonio']\n",
    "for s in test_mention_strings:\n",
    "    test_word_missed_status_idx = [i for i,t in enumerate(lexicon_missed_tags) if s in t]\n",
    "    status_sample_lower = map(lambda x: x.lower(), status_sample)\n",
    "    test_word_missed_status = [status_sample_lower[i] for i in test_word_missed_status_idx]\n",
    "    print('test mention string \"%s\" not detected in the following statuses'%(s))\n",
    "    print(test_word_missed_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! So the errors are probably coming from tokenization errors: \n",
    "\n",
    "The string `palmas bajas-guayama` should be split into `[palmas, bajas, guayama]` but is instead split into `[palmas, bajas-guayama]` because there is no hyphen splitting rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "['am', 'damaged', 'facebook', 'fam', 'government', 'papi', 'salvacion']\n"
     ]
    }
   ],
   "source": [
    "joint_false_tags = set(ner_false_tags) & set(lexicon_false_tags)\n",
    "print(len(joint_false_tags))\n",
    "print(sorted(joint_false_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shared false tags are mostly common words that shouldn't be in the lexicon.\n",
    "\n",
    "The other false tags must diverse significantly for the NER and lexicon methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NER only missed tags = 272\n",
      "['19 Calle', '65th Infantry', 'Adjuntas', 'Aguadilla Airport', 'Aguas Buenas', 'Aguirre', 'Aguirre, PR', 'Aibonito', 'Algarrobo', 'Anasco', 'Areciblo Pueblo', 'Arecibo', 'Arroyo', 'Atenas College', 'Av. Franklin Delano Roosevelt', 'Ave Barbosa', 'Ave Baxter', 'Ave Domenech', 'Ave Hostos', 'Ave Lomas Verdes', 'Ave Los Dominicos', 'Ave Los Patriotas', 'Ave Magnolia', 'Ave Maruca', 'Ave Pedro Albizu Campos', 'Ave Ponce de Leon', 'Ave Ramon L Rodriguez', 'Ave Ramon Luis Rivera', 'Ave Roberto H Todd', 'Ave. De Diego', 'Ave. Irlanda Height', 'Ave. Los Veteranos', 'Ave. Montecarlo', 'Ave. Santa Juanita', 'BARRIO CORAZON', 'Bairoa', 'Barceloneta', 'Barreras', 'Barriada Santa Ana', 'Barrio Brandeis', 'Barrio Corazon', 'Barrio Linea Capo', 'Barrio Olimpo', 'Barrio Pueblo', 'Barrio Singapur', 'Barrio corazon', 'Barrios Obrero', 'Bayamon', 'Bayamon Gardens', 'Bda Santa Ana', 'Blondet', 'Blvd Dr.', 'Bo candelaria', 'Bo. Carite', 'Bomberos de Guayama', 'Boqueron', 'Borinquen', 'CALLE LAS FLORES', 'Cabo Rojo', 'Calle 2', 'Calle 3', 'Calle 4', 'Calle 46', 'Calle 615', 'Calle Alondra', 'Calle Aracibo', 'Calle Arzuaga', 'Calle B', 'Calle C', 'Calle Casia', 'Calle Dr. Vidal', 'Calle E', 'Calle Enrique Gonzalez', 'Calle Girasol', 'Calle Hostos', 'Calle Igualdad', 'Calle Jazmin', 'Calle Juan Calaf', 'Calle Luis Munoz Rivera', 'Calle Marginal', 'Calle Morales Bo Hato Tejas', 'Calle Navarra', 'Calle Obispado', 'Calle San Claudio', 'Calle San Millan', 'Calle Tartak', 'Calle Villa', 'Camuy', 'Caparra Heights', 'Caribbean', 'CariterGuayama', 'Carolina', 'Carolina Pueblo', 'Carolina Shoppin Ctr', 'Carr 122', 'Carr 159', 'Carr 2', 'Carr Estatal', 'Carr. 829', 'Carr. 845']\n",
      "lexicon only missed tags = 205\n",
      "['19 calle', '65th infantry', 'adjuntas', 'aguirre', 'aguirre, pr', 'anasco', 'apartamento santa rosa', 'areciblo pueblo', 'atenas college', 'av. franklin delano roosevelt', 'ave barbosa', 'ave baxter', 'ave domenech', 'ave hostos', 'ave lomas verdes', 'ave los dominicos', 'ave los patriotas', 'ave magnolia', 'ave maruca', 'ave pedro albizu campos', 'ave ponce de leon', 'ave ramon l rodriguez', 'ave ramon luis rivera', 'ave roberto h todd', 'ave. de diego', 'ave. irlanda height', 'ave. los veteranos', 'ave. montecarlo', 'ave. santa juanita', 'bairoa', 'barceloneta', 'barreras', 'barriada santa ana', 'barrio brandeis', 'barrio de carite', 'barrio linea capo', 'barrio olimpo', 'barrio pueblo', 'barrio singapur', 'barrios obrero', 'bayamon gardens', 'bda santa ana', 'blvd dr.', 'bo candelaria', 'bo. carite', 'bomberos de guayama', 'boqueron', 'cabo rojo', 'calle 3', 'calle 4', 'calle 615', 'calle a', 'calle alondra', 'calle aracibo', 'calle arzuaga', 'calle b', 'calle c', 'calle casia', 'calle dr. vidal', 'calle e', 'calle enrique gonzalez', 'calle francisco', 'calle girasol', 'calle hostos', 'calle igualdad', 'calle jazmin', 'calle juan calaf', 'calle las flores', 'calle luis munoz rivera', 'calle marginal', 'calle morales bo hato tejas', 'calle navarra', 'calle obispado', 'calle palestina', 'calle san claudio', 'calle san jorge', 'calle san millan', 'calle tartak', 'calle villa', 'camuy', 'caparra heights', 'cariterguayama', 'carolina pueblo', 'carolina shoppin ctr', 'carr 122', 'carr 159', 'carr 2', 'carr estatal', 'carr. 829', 'carr. 845', 'carrioca', 'casa paola', 'ceiba airport', 'centro de manejo de emergencias', 'christainsted', 'ciales', 'cidra', 'cimarrona', 'comercial bonsai plaza', 'comerio']\n"
     ]
    }
   ],
   "source": [
    "ner_only_missed_tags = set(ner_missed_tags_list) - set(lexicon_missed_tags_list)\n",
    "lexicon_only_missed_tags = set(lexicon_missed_tags_list) - set(ner_missed_tags_list)\n",
    "print('NER only missed tags = %d'%(len(ner_only_missed_tags)))\n",
    "print(sorted(ner_only_missed_tags)[:100])\n",
    "print('lexicon only missed tags = %d'%(len(lexicon_only_missed_tags)))\n",
    "print(sorted(lexicon_only_missed_tags)[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lexicon_false_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outside of the very obscure mentions (`calle igualdad`, `apartamento santa rosa`), there are words in the lexicon-only missed tags that should not have been missed:\n",
    "\n",
    "`aguirre`, `ceiba airport`, `cidra`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print('cidra' in location_mention_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mention string \"aguirre\" not detected the following statuses\n",
      "[u'last updated oct. 1, 2017, 7 p.m. etoct. 1, 2017, 7 p.m. ethurricane mariacaribbeanmonday will see more mail operations across puerto rico and the virgin islands as inbound mail from the mainland continues to be transported and processed across the caribbean.   postal employees are in full force providing mail and postal retail services for customers in postal facilities that were not significantly damaged by hurricane maria.if an office is damaged, postal officials are designating nearby locations for customers to receive mail. in some cases in both puerto rice and the virgin islands, letter carriers have begun delivery where safe.puerto ricopartial delivery has started today where safe to do so for the following locations:65th infantry, station, 100 calle alondra, 00923.bayamon, branch, 100 ave ramon l rodriguez, 00956.cupey, station, 369 calle san claudio frnt, 00926.hato rey, station, 361 calle juan calaf, 00917.old san juan, station, 100 paseo de colon ste 1, 00901.rio piedras, station, 112 calle arzuaga ste 1, 00928.vieques, main po, 97 calle luis munoz rivera ste 103, 00765.customers served by the following offices can pick up their mail at the alternate locations listed.office      alternate office   aibonito, 00705.    cayey, 00736.     areciblo pueblo, 00612.    arecibo mpo, 00613.   arroyo, 00714.      patillas, 00723.   bayamon gardens, 00957.  bayamon branch, 00956.   canovanas, 00729.    rio grande, 00745.   caparra heights, 00968.    hato rey, 00917.   cotto, 00612.      arecibo mpo, 00613.   fernandez juncos, 00909.  santurce station, 00907. guayama, 00784.    salinas, 00751.     juncos, 00777.      gurabo, 00778.     la plata, 00786.    cayey, 00736.     luquillo, 00773.    fajardo, 00738.   plaza carolina, 00985.    carolina mpo, 00983.   plazas las americas, 00918.  san juan gpo, 00953.   punta santiago, 00741.    humacoa, 00741.   santa isabel, 00757.    coamo, 00769.     trujillo alto, 00976.    65th infantry, 00924    the following offices will be open for retail, po box mail and package pickup, starting at 9 a.m.   regular closing hours will be observed.adjuntas, 00601.**aguirre**, 00704.anasco, 00610.arecibo, 00612.barceloneta, 00617.boqueron, 00622.cabo rojo, 00623.camuy, 00627.carolina, 00979.carolina pueblo, 00986.catano, 00962.cayey, 00736.ciales, 00638.coamo, 00762.comerio, 00782.cidra, 00739.dorado, 00646.guaynabo, 00966.lajas, 00667.lares, 00669.maricao, 00606.marina, 00680.mayaguez, 00680.minillas center, 00940.moca, 00676.naguabo, 00718.north, 00726.patillas, 00723.ponce, 00716.ramey, 00604.rincon, 00677.sabana grande, 00637.sabana hoyos, 00688.san antonio, 00690.san german, 00683.san juan, 00933.san sebastian, 00685.santurce, 00908.toa baja, 00949.utuado, 00641.vega alta, 00692.vega baja, 00693victoria, 00605.san juan airport retail, 00937.the following offices are open po box mail and package pickup, starting at 9 a.m. regular closing hours will be observed. aguas buenas, 00703. san juan amf, 00985. ceiba, 00735. corozal, 00783. gurabo, 00778. naranjito, 00719. orocovis, 00720. barrios obrero, 00915.the san juan p&dc and catano dmdu annex are closed and not accepting fast appointments. st. thomasretail services and mail and package pickup tomorrow, sunday, oct. 1, will occur during regular sunday hours. offices open for mail and package pickup remain the same.east end /tutu mall, 00805. charlotte amalie, mail handout service, 00801, 00803, 00805. street addresses, 00802.customers served by the following offices can pick up their mail at the alternate locations.regular office  alternate   veterans annex, 00802.  charlotte amalie, 00801.emancipation gardens, 00802.charlotte amalie, 00801. st. johncustomers service by the following officecan pick up their mail at the alternate location.regular    alternate      cruz bay, 00830.  trucks in front of building st. croixthe following offices will be open for retail, po box mail and package pickup. regular sunday hours will be observed.christainsted, 00820. downtown, 00820.gallows bay, 00824. sunny isle, 00823.   customers service by the following office can pick up their mail at the alternate location.regular  alternate   kinghill, 00840.  frederiksted, 00850.               usps is not accepting priority mail express service or shipment of live animals for the following zip codes: 006, 007, 008 and 009. the following island is closed for retail services, package or mail pickup.culebra, 00775.']\n",
      "test mention string \"ceiba airport\" not detected the following statuses\n",
      "[u'puerto rico pharmacies operating as of 9/22/2017cvs pharmacy pr-2 km 77.6, calle san millan arecibocvs pharmacy calle morales bo hato tejas bayamoncvs pharmacy pr 167 int pr 829, ave magnolia bayamoncvs pharmacy #600 urb. flamboy\\xe1n grdns bayamoncvs pharmacy rexville plaza bayamoncvs pharmacy calle gautier ben\\xedtez caguascvs pharmacy 830 carr 857-calle marginal carolinacvs pharmacy ave. de diego-carolina shoppin ctr carolinacvs pharmacy calle tartak carolinacvs pharmacy carr 696 - ave pedro albizu campos doradocvs pharmacy #9 calle igualdad fajardocvs pharmacy 178 carr 2 - frente a crossfit pr guaynabocvs pharmacy 88 calle 2 - frente a atenas college manaticvs pharmacy 399 calle villa poncecvs pharmacy urb. jardines de fagot, calle obispado poncecvs pharmacy 2511 ponce bypass-al lado de mcdonalds poncecvs pharmacy 105 paseo gilberto concepci\\xf3n san juancvs pharmacy 60 ave los dominicos toa bajacvs pharmacy 6109 carr 694 - frente burger king vega altacvs pharmacy 3950 carr-2 vega bajafarmacia borinquen dr. barreras & corchado juncosfarmacia caney 1 a-9 calle aracibo, villas caney trujillo altofarmacia caridad 5 #21 calle 615 blq 237, villa carolina carolinafarmacia caridad 7 ave lomas verdes san juanfarmacia menaa 301 ave barbosa san juanfarmacia san jose 7878529494 calle dr. vidal humacaofarmacias plaza ave domenech san juansan juan vamc pharmacy hosp de vets calle casia #10 san juansu farmacia amiga inc. bairoa caguaswalgreens carr-2 plaza victoria aguadillawalgreens pr-2 & pr 402 - bo caracol anascowalgreens 1000 ave ramon luis rivera bayamonwalgreens inter carr 122 & ave baxter san germanwalgreens  carr 159 km 153 barrio pueblo corozalwalgreens  pr-3 km 42.76 - comercial bonsai plaza fajardowalgreens  1 calle marginal guayamawalgreens  25 ave ponce de leon/corner pr24 guaynbo catanowalgreens  379 ave los patriotas lareswalgreens  175 carr pr 385 penuelaswalgreens  2706 ave maruca poncewalgreens  1800 calle navarra poncewalgreens  65 infanter\\xeda shopping ctr rio piedraswalgreens  700 ave roberto h todd san juanwalgreens  999 ave mu\\xf1oz rivera san juanwalgreens  100 blvd dr. santa rosa bayamonwalgreens  urb. plaza encantada  trujillo altowalgreens  calle p\\xfablica & pr-2 vega bajawal-mart pharmacy division ave. los veteranos carr#3 guayamawal-mart pharmacy division 975 ave hostos mayaguezwal-mart pharmacy division carr estatal #3 poncetjzs/san juan fir atc operating but with reduced capacity, limitations to staff and comms.tjsj/san juan airport closed, once operating will be restricted to relieft flights until 30 sep. no cpdlc, vor or radar coverage at the moment. tjsj auto weather lost after winds peak gust went above 91 knots. terminal damaged, much flooding inside.tjig/isla grande images show as many as 20 aircraft destroyed. some terminal buildings and many hangars damaged beyond repair tjbq/aguadilla airport closed, earliest reopening friday evening.tjps/ponce airport closed, earliest reopening friday evening.tjrv/**ceiba airport** closed. next update expected 22 sep 1600z.tjvq/vieques airport closed. next update expected 22 sep 1600z.']\n",
      "test mention string \"cidra\" not detected the following statuses\n",
      "[u'last updated oct. 1, 2017, 7 p.m. etoct. 1, 2017, 7 p.m. ethurricane mariacaribbeanmonday will see more mail operations across puerto rico and the virgin islands as inbound mail from the mainland continues to be transported and processed across the caribbean.   postal employees are in full force providing mail and postal retail services for customers in postal facilities that were not significantly damaged by hurricane maria.if an office is damaged, postal officials are designating nearby locations for customers to receive mail. in some cases in both puerto rice and the virgin islands, letter carriers have begun delivery where safe.puerto ricopartial delivery has started today where safe to do so for the following locations:65th infantry, station, 100 calle alondra, 00923.bayamon, branch, 100 ave ramon l rodriguez, 00956.cupey, station, 369 calle san claudio frnt, 00926.hato rey, station, 361 calle juan calaf, 00917.old san juan, station, 100 paseo de colon ste 1, 00901.rio piedras, station, 112 calle arzuaga ste 1, 00928.vieques, main po, 97 calle luis munoz rivera ste 103, 00765.customers served by the following offices can pick up their mail at the alternate locations listed.office      alternate office   aibonito, 00705.    cayey, 00736.     areciblo pueblo, 00612.    arecibo mpo, 00613.   arroyo, 00714.      patillas, 00723.   bayamon gardens, 00957.  bayamon branch, 00956.   canovanas, 00729.    rio grande, 00745.   caparra heights, 00968.    hato rey, 00917.   cotto, 00612.      arecibo mpo, 00613.   fernandez juncos, 00909.  santurce station, 00907. guayama, 00784.    salinas, 00751.     juncos, 00777.      gurabo, 00778.     la plata, 00786.    cayey, 00736.     luquillo, 00773.    fajardo, 00738.   plaza carolina, 00985.    carolina mpo, 00983.   plazas las americas, 00918.  san juan gpo, 00953.   punta santiago, 00741.    humacoa, 00741.   santa isabel, 00757.    coamo, 00769.     trujillo alto, 00976.    65th infantry, 00924    the following offices will be open for retail, po box mail and package pickup, starting at 9 a.m.   regular closing hours will be observed.adjuntas, 00601.aguirre, 00704.anasco, 00610.arecibo, 00612.barceloneta, 00617.boqueron, 00622.cabo rojo, 00623.camuy, 00627.carolina, 00979.carolina pueblo, 00986.catano, 00962.cayey, 00736.ciales, 00638.coamo, 00762.comerio, 00782.**cidra**, 00739.dorado, 00646.guaynabo, 00966.lajas, 00667.lares, 00669.maricao, 00606.marina, 00680.mayaguez, 00680.minillas center, 00940.moca, 00676.naguabo, 00718.north, 00726.patillas, 00723.ponce, 00716.ramey, 00604.rincon, 00677.sabana grande, 00637.sabana hoyos, 00688.san antonio, 00690.san german, 00683.san juan, 00933.san sebastian, 00685.santurce, 00908.toa baja, 00949.utuado, 00641.vega alta, 00692.vega baja, 00693victoria, 00605.san juan airport retail, 00937.the following offices are open po box mail and package pickup, starting at 9 a.m. regular closing hours will be observed. aguas buenas, 00703. san juan amf, 00985. ceiba, 00735. corozal, 00783. gurabo, 00778. naranjito, 00719. orocovis, 00720. barrios obrero, 00915.the san juan p&dc and catano dmdu annex are closed and not accepting fast appointments. st. thomasretail services and mail and package pickup tomorrow, sunday, oct. 1, will occur during regular sunday hours. offices open for mail and package pickup remain the same.east end /tutu mall, 00805. charlotte amalie, mail handout service, 00801, 00803, 00805. street addresses, 00802.customers served by the following offices can pick up their mail at the alternate locations.regular office  alternate   veterans annex, 00802.  charlotte amalie, 00801.emancipation gardens, 00802.charlotte amalie, 00801. st. johncustomers service by the following officecan pick up their mail at the alternate location.regular    alternate      cruz bay, 00830.  trucks in front of building st. croixthe following offices will be open for retail, po box mail and package pickup. regular sunday hours will be observed.christainsted, 00820. downtown, 00820.gallows bay, 00824. sunny isle, 00823.   customers service by the following office can pick up their mail at the alternate location.regular  alternate   kinghill, 00840.  frederiksted, 00850.               usps is not accepting priority mail express service or shipment of live animals for the following zip codes: 006, 007, 008 and 009. the following island is closed for retail services, package or mail pickup.culebra, 00775.']\n"
     ]
    }
   ],
   "source": [
    "test_mention_strings = ['aguirre', 'ceiba airport', 'cidra']\n",
    "for s in test_mention_strings:\n",
    "    test_word_missed_status_idx = [i for i,t in enumerate(lexicon_missed_tags) if s in t]\n",
    "    status_sample_lower = map(lambda x: x.lower(), status_sample)\n",
    "    test_word_missed_status = [status_sample_lower[i].replace(s, '**%s**'%(s)) for i in test_word_missed_status_idx]\n",
    "    print('test mention string \"%s\" not detected the following statuses'%(s))\n",
    "    print(test_word_missed_status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More tokenization errors. This suggests that we should build a custom function using TweetTokenizer plus a few extra rules involving slashes and dashes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entity linking baseline: wiki links\n",
    "We need a baseline for entity linking. We should redo the annotation and add the actual Wiki links where possible, then use this to determine the precision/recall (@k) of the entity candidates generated by the CrossWikis lexicon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...many hours later..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's count how many of the annotated entities actually have Wiki pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Guayama', 'https://en.wikipedia.org/wiki/Guayama,_Puerto_Rico')]\n"
     ]
    }
   ],
   "source": [
    "annotate_matcher = re.compile('<<([\\w\\s\\,\\.]+)>>\\[\\[(\\S*)\\]\\]')\n",
    "test = \"\"\"\n",
    "*PORFAVOR*** Muy PREOCUPADA***por mi Querida Tia &Tio- Carlos (Jun) Vega Y Rosa Vega- Estancias el Vijia en <<Guayama>>[[https://en.wikipedia.org/wiki/Guayama,_Puerto_Rico]] . Rosanna Rivera de Orlando- sabe algo porfavor contact me.\n",
    "\"\"\"\n",
    "print(annotate_matcher.findall(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "annotate_matcher = re.compile('<<([\\w\\s\\,\\.]+)>>\\[\\[(\\S+)\\]\\]')\n",
    "data_dir = '../../data/facebook-maria/'\n",
    "group_id = 1773209126315380\n",
    "sample_annotated_file = os.path.join(data_dir, '%s_post_sample_annotated_wiki.txt'%(group_id))\n",
    "final_line = 249\n",
    "sample_annotated_wiki_txt = [l.strip() for i, l in enumerate(codecs.open(sample_annotated_file)) if i < final_line]\n",
    "sample_annotated_wiki_txt_mentions = [annotate_matcher.findall(l) for l in sample_annotated_wiki_txt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Guayama', 'https://en.wikipedia.org/wiki/Guayama,_Puerto_Rico')], [('olimpo', 'NIL'), ('calle 6', 'NIL'), ('calle 8', 'NIL')], [('Guayama', 'https://en.wikipedia.org/wiki/Guayama,_Puerto_Rico'), ('residencial Villamar', 'NIL')], [('Guayama', 'https://en.wikipedia.org/wiki/Guayama,_Puerto_Rico'), ('Carite', 'NIL')], [('calle 54', 'NIL')], [('Guayma', 'https://en.wikipedia.org/wiki/Guayama,_Puerto_Rico'), ('villa Rosa', 'NIL'), ('calle 2', 'NIL')], [('Guayama URB', 'https://en.wikipedia.org/wiki/Guayama,_Puerto_Rico'), ('Hacienda AS', 'NIL'), ('19 Calle', 'NIL')], [], [], [('barrio mosquito', 'https://es.wikipedia.org/wiki/Mosquito_(Vieques)')]]\n"
     ]
    }
   ],
   "source": [
    "print(sample_annotated_wiki_txt_mentions[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328 mentions total\n",
      "53.354% valid mentions\n"
     ]
    }
   ],
   "source": [
    "wiki_mentions_collapsed = reduce(lambda x,y: x+y, sample_annotated_wiki_txt_mentions)\n",
    "print('%d mentions total'%(len(wiki_mentions_collapsed)))\n",
    "non_nil_wiki_mentions = filter(lambda x: x[1]!='NIL', wiki_mentions_collapsed)\n",
    "non_nil_wiki_mention_pct = len(non_nil_wiki_mentions) / len(wiki_mentions_collapsed) * 100\n",
    "print('%.3f%% valid mentions'%(non_nil_wiki_mention_pct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! About half the mentions are valid. What are some of the invalid mentions?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('olimpo', 'NIL'), ('calle 6', 'NIL'), ('calle 8', 'NIL'), ('residencial Villamar', 'NIL'), ('Carite', 'NIL'), ('calle 54', 'NIL'), ('villa Rosa', 'NIL'), ('calle 2', 'NIL'), ('Hacienda AS', 'NIL'), ('19 Calle', 'NIL'), ('Vives', 'NIL'), ('Los Barros', 'NIL'), ('la carr 15', 'NIL'), ('La Gran Parada', 'NIL'), ('villa Rosa', 'NIL'), ('calle 2', 'NIL'), ('las mareas', 'NIL'), ('Jardines de Guamani', 'NIL'), ('Cimarrona', 'NIL'), ('Jardines de guamani', 'NIL')]\n"
     ]
    }
   ],
   "source": [
    "nil_wiki_mentions = filter(lambda x: x[1]=='NIL', wiki_mentions_collapsed)\n",
    "print(nil_wiki_mentions[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostly obscure neighborhoods (`olimpo`) and roads (`calle 2`). Maybe I shouldn't have marked those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Guayama', 'https://en.wikipedia.org/wiki/Guayama,_Puerto_Rico')], [], [('Guayama', 'https://en.wikipedia.org/wiki/Guayama,_Puerto_Rico')], [('Guayama', 'https://en.wikipedia.org/wiki/Guayama,_Puerto_Rico')], []]\n"
     ]
    }
   ],
   "source": [
    "non_nil_wiki_mentions = map(lambda x: filter(lambda y: y[1]!='NIL', x), sample_annotated_wiki_txt_mentions)\n",
    "print(non_nil_wiki_mentions[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what recall/precision are like for NER and lexicon with just the Wiki mentions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['guayama'], [], ['guayama'], ['guayama'], [], ['guayma'], ['guayama urb'], [], [], ['barrio mosquito']]\n"
     ]
    }
   ],
   "source": [
    "# just need the text for accuracy\n",
    "non_nil_wiki_mentions_txt = map(lambda x: map(lambda y: y[0], x), non_nil_wiki_mentions)\n",
    "non_nil_wiki_mentions_txt_lower = map(lambda x: map(lambda y: y.lower(), x), non_nil_wiki_mentions_txt)\n",
    "print(non_nil_wiki_mentions_txt_lower[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_false_tags, ner_missed_tags, ner_precision, ner_recall = test_precision_recall(status_sample_ner_entity_list[:N], non_nil_wiki_mentions_txt)\n",
    "lexicon_false_tags, lexicon_missed_tags, lexicon_precision, lexicon_recall = test_precision_recall(status_sample_lexicon_entities_lower[:N], non_nil_wiki_mentions_txt_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on Wiki annotations:\n",
      "NER precision=0.072, recall=0.389\n",
      "lexicon precision=0.025, recall=0.825\n"
     ]
    }
   ],
   "source": [
    "print('on Wiki annotations:')\n",
    "print('NER precision=%.3f, recall=%.3f'%(ner_precision, ner_recall))\n",
    "print('lexicon precision=%.3f, recall=%.3f'%(lexicon_precision, lexicon_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice!! So lexicon gets pretty much every single Wiki-linked entity. Probably because half of them are `Guayama` BUT STILL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual entity linking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each anchor string we've computed the entity with the maximum conditional probability, based on the CrossWikis probabilities:\n",
    "\n",
    "$$argmax_{e} \\: P(s | e)$$\n",
    "\n",
    "So now let's compute the highest possible precision: assuming that the Wiki-searchable entities were perfectly extracted, what percentage of the entities are correctly disambiguated using the entity with the max probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bz2 import BZ2File\n",
    "# LIMIT TO DATASET to avoid memory sadness\n",
    "full_mention_set = set(reduce(lambda x,y: x+y, status_sample_ner_entity_list[:N]))\n",
    "full_mention_set_lower = set(reduce(lambda x,y: x+y, status_sample_lexicon_entities_lower[:N]))\n",
    "prob_file = '/hg190/corpora/crosswikis-data.tar.bz2/max_prob_entities.bz2'\n",
    "max_prob_entities = []\n",
    "max_prob_entities_lower = []\n",
    "for i, l in enumerate(BZ2File(prob_file, 'r')):\n",
    "    # skip header line\n",
    "    if(i > 0):\n",
    "        mention, prob, entity = l.strip().split('\\t')\n",
    "        prob = float(prob)\n",
    "        mention_lower = mention.lower()\n",
    "        if(mention in full_mention_set):\n",
    "            max_prob_entities.append([mention, prob, entity])\n",
    "        elif(mention_lower in full_mention_set_lower):\n",
    "            max_prob_entities_lower.append([mention_lower, prob, entity])\n",
    "col_names = ['mention', 'prob', 'entity']\n",
    "max_prob_entities = pd.DataFrame(max_prob_entities, columns=col_names)\n",
    "max_prob_entities_lower = pd.DataFrame(max_prob_entities_lower, columns=col_names)\n",
    "max_prob_entities.set_index('mention', inplace=True)\n",
    "max_prob_entities_lower.set_index('mention', inplace=True)\n",
    "# DON'T USE PANDAS it kills the memory ;_;\n",
    "# max_probs = pd.read_csv(prob_file, sep='\\t', compression='bz2')\n",
    "# also convert to lowercase because that might work better\n",
    "# compute max prob over lowercase\n",
    "# max_probs_lower_idx = map(str.lower, max_probs.index)\n",
    "# max_probs_lower = max_probs.copy()\n",
    "# max_probs_lower.index = max_probs_lower_idx\n",
    "# max_probs_lower = max_probs_lower.groupby(max_probs_lower_idx).apply(lambda x: x[x.loc[:, 'prob'] == x.loc[:, 'prob'].max()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from itertools import izip\n",
    "UNK = 'UNK'\n",
    "def guess_most_likely_entities(wiki_mentions, max_probs):\n",
    "    most_likely_entities = []\n",
    "    for m_list in wiki_mentions:\n",
    "        e_list = []\n",
    "        for m in m_list:\n",
    "            if(m in max_probs):\n",
    "                e = max_probs.loc[m, 'entity']\n",
    "            else:\n",
    "                e = UNK\n",
    "            e_list.append(e)\n",
    "        most_likely_entities.append(e_list)\n",
    "    return most_likely_entities\n",
    "\n",
    "def compute_linking_precision(gold_entities, est_entities):\n",
    "    tp = sum([len([1 if g==e else 0 for g, e in izip(g_list, e_list)]) for g_list,e_list in izip(gold_entities, est_entities)])\n",
    "    fp = sum([len([1 if g!=e else 0 for g, e in izip(g_list, e_list)]) for g_list,e_list in izip(gold_entities, est_entities)])\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "est_entities = guess_most_likely_entities(non_nil_wiki_mentions_txt, max_probs)\n",
    "non_nil_wiki_mentions_entities = map(lambda x: map(lambda y: y[1], x), non_nil_wiki_mentions)\n",
    "link_precision = compute_linking_precision(non_nil_wiki_mentions_entities, est_entities)\n",
    "print('naive link precision with max probs = %.3f'%(link_precision))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
