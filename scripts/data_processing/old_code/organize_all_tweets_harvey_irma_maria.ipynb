{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize all tweets Harvey, Irma and Maria\n",
    "We need to standard the format of all our tweets because some are JSON and others are .tsv format. LET'S DO DIS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desired format:\n",
    "\n",
    "- row = tweet\n",
    "- columns = tweet ID, time created, text, favorites, place, retweets, user name, user id, user place, user bio, retweet data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "DATE_FMT = '%a %b %d %H:%M:%S +0000 %Y'\n",
    "def extract_data_from_json(tweet_data):\n",
    "    \"\"\"\n",
    "    Extract relevant data from json object.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    tweet_data : dict\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tweet_data_extract : dict\n",
    "    \"\"\"\n",
    "    tweet_data_extract = {}\n",
    "    tweet_data_extract['id'] = tweet_data['id']\n",
    "    tweet_data_extract['created_at'] = datetime.strptime(tweet_data['created_at'], DATE_FMT)\n",
    "    tweet_data_extract['text'] = tweet_data['text']\n",
    "    tweet_data_extract['lang'] = tweet_data['lang']\n",
    "    tweet_data_extract['favorites'] = tweet_data['favorite_count']\n",
    "    tweet_data_extract['retweets'] = tweet_data['retweet_count']\n",
    "    if(tweet_data.get('place')):\n",
    "        tweet_data_extract['place'] = tweet_data['place']['full_name']\n",
    "        tweet_data_extract['place.type'] = tweet_data['place']['place_type']\n",
    "    user_data = tweet_data['user']\n",
    "    tweet_data_extract['user.name'] = user_data['screen_name']\n",
    "    tweet_data_extract['user.id'] = user_data['id']\n",
    "    tweet_data_extract['user.bio'] = user_data['description']\n",
    "    tweet_data_extract['user.followers'] = user_data['followers_count']\n",
    "    tweet_data_extract['user.friends'] = user_data['friends_count']\n",
    "    tweet_data_extract['user.lang'] = user_data['lang']\n",
    "    tweet_data_extract['user.statuses'] = user_data['statuses_count']\n",
    "    tweet_data_extract['user.place'] = user_data['location']\n",
    "    if(tweet_data.get('retweeted_status')):\n",
    "        rt_data = tweet_data['retweeted_status']\n",
    "        rt_data = extract_data_from_json(rt_data)\n",
    "        rt_data = {'retweeted.%s'%(k) : v for k,v in rt_data.iteritems()}\n",
    "        tweet_data_extract.update(rt_data)\n",
    "    return tweet_data_extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Harvey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 0 tweets\n",
      "processed 1000000 tweets\n",
      "processed 2000000 tweets\n",
      "processed 3000000 tweets\n",
      "processed 4000000 tweets\n",
      "processed 5000000 tweets\n",
      "processed 6000000 tweets\n",
      "error with line 911233687156600839\n",
      "error with line 911233742810775553\n",
      "error with line 911233811282788352\n",
      "error with line 911233911723892736\n",
      "error with line 911234040874848257\n",
      "error with line 911234342311092229\n",
      "error with line 911234344743768065\n",
      "error with line 911234348380295168\n",
      "error with line 911234356542394369\n",
      "error with line 911234500239249409\n",
      "error with line 911234905979449346\n",
      "error with line 911234991937474561\n",
      "error with line 911235154303242241\n",
      "error with line 911235506075299840\n",
      "error with line 911235788549115904\n",
      "error with line 911235883831054336\n",
      "error with line 911235929649684482\n",
      "error with line 911236042539372545\n",
      "error with line 911236061413724160\n",
      "error with line 911236113095970818\n",
      "error with line 911236342767669248\n",
      "error with line 911236412388859905\n",
      "error with line 911236549823664131\n",
      "error with line 911236918972698624\n",
      "error with line 911236924546981888\n",
      "error with line 911237181041184771\n",
      "error with line 911237212431405057\n",
      "error with line 911237695753646080\n",
      "error with line 911237784723128320\n",
      "error with line 911237950393978881\n",
      "error with line 911238244301500417\n",
      "error with line 911238263163285506\n",
      "error with line 911238298978390017\n",
      "error with line 911238478171656193\n",
      "error with line 911238515459072001\n",
      "error with line 911238804727623680\n",
      "error with line 911238966485143552\n",
      "error with line 911238969937055744\n",
      "error with line 911238973065920514\n",
      "error with line 911239154280919040\n",
      "error with line 911239269573971970\n",
      "error with line 911239672856064000\n",
      "error with line 911239833485479937\n",
      "error with line 911239865244749825\n",
      "error with line 911239969372504064\n",
      "error with line 911240029363736577\n",
      "error with line 911240351117127681\n",
      "error with line 911240363020517376\n",
      "error with line 911240372042399745\n",
      "error with line 911240722422083585\n",
      "error with line 911240724183691264\n",
      "error with line 911240870468481024\n",
      "error with line 911240991134404610\n",
      "error with line 911241297729658881\n",
      "error with line 911241322572414976\n",
      "error with line 911241330445008896\n",
      "error with line 911241357762744322\n",
      "error with line 911241423630082049\n",
      "error with line 911241592224096257\n",
      "error with line 911241649514262529\n",
      "error with line 911241901570846720\n",
      "error with line 911241947137826816\n",
      "error with line 911242000480993280\n",
      "error with line 911242110929707011\n",
      "error with line 911242127522353152\n",
      "error with line 911242132521750528\n"
     ]
    }
   ],
   "source": [
    "# start with easy file\n",
    "doc_now_data_file = '../../data/mined_tweets/HurricaneHarvey_ids_rehydrated.json.gz'\n",
    "harvey_data_list_1 = []\n",
    "for i, l in enumerate(gzip.open(doc_now_data_file, 'r')):\n",
    "    l_json = json.loads(l.strip())\n",
    "    try:\n",
    "        l_data = extract_data_from_json(l_json)\n",
    "        harvey_data_list_1.append(l_data)\n",
    "    except Exception, e:\n",
    "        print('error with line %s'%(l_json))\n",
    "    if(i % 1000000 == 0):\n",
    "        print('processed %d tweets'%(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# combine\n",
    "harvey_data_df_1 = pd.concat([pd.Series(t) for t in harvey_data_list_1], axis=1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1000 tweets\n",
      "processed 2000 tweets\n",
      "processed 3000 tweets\n",
      "processed 4000 tweets\n",
      "processed 5000 tweets\n",
      "processed 6000 tweets\n",
      "processed 7000 tweets\n",
      "processed 8000 tweets\n",
      "processed 9000 tweets\n",
      "processed 10000 tweets\n",
      "processed 11000 tweets\n",
      "processed 12000 tweets\n",
      "processed 13000 tweets"
     ]
    }
   ],
   "source": [
    "# now add the individual JSON files (from archive)\n",
    "import re\n",
    "import os\n",
    "from ast import literal_eval\n",
    "data_dir = '../../data/mined_tweets/'\n",
    "file_keyword = '#harvey'\n",
    "file_matcher = re.compile('archive.*%s.*.gz'%(file_keyword))\n",
    "harvey_json_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if file_matcher.match(f.lower())]\n",
    "harvey_data_list_2 = []\n",
    "tweet_keyword = 'harvey'\n",
    "tweet_matcher = re.compile('.*%s.*'%(tweet_keyword))\n",
    "tweet_ctr = 0\n",
    "for f in harvey_json_files:\n",
    "    for l in gzip.open(f, 'r'):\n",
    "        # some JSON decoding error...WHY\n",
    "#         try:\n",
    "#             l = l.strip().decode('utf-8')\n",
    "#             l_json = json.loads(l)\n",
    "#         except Exception, e:\n",
    "#             print('error at line %s'%(l))\n",
    "#             break\n",
    "        l_json = literal_eval(l.strip())\n",
    "        if(tweet_matcher.match(l_json['text'].lower())):\n",
    "            l_data = extract_data_from_json(l_json)\n",
    "            harvey_data_list_2.append(l_data)\n",
    "            tweet_ctr += 1\n",
    "            if(len(harvey_data_list_2) % 1000 == 0):\n",
    "                print('processed %d tweets'%(len(harvey_data_list_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "harvey_data_df_2 = pd.concat([pd.Series(t) for t in harvey_data_list_2], axis=1).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_historical_df_to_standard(df):\n",
    "    \"\"\"\n",
    "    Fix data columns to match standard.\n",
    "    \"\"\"\n",
    "    date_fmt = '%Y-%m-%d %H:%M'\n",
    "    df_new = df.copy()\n",
    "    # fix columns\n",
    "    df_new.rename(columns={'username' : 'user.name', 'date' : 'created_at', 'geo' : 'place'}, inplace=True)\n",
    "    df_new.drop(['mentions', 'hashtags','permalink'], axis=1, inplace=True)\n",
    "    # fix date\n",
    "    df_new = df_new.assign(created_at=df_new.loc[:, 'created_at'].apply(lambda x: datetime.strptime(x, date_fmt)))\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# now add the individual .tsv files...which should really be JSON format for #maximumData\n",
    "file_matcher = re.compile('(?<!archive)#.*harvey.*.gz')\n",
    "tweet_keyword = 'harvey'\n",
    "tweet_matcher = re.compile('.*%s.*'%(tweet_keyword))\n",
    "harvey_tsv_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if file_matcher.match(f.lower())]\n",
    "harvey_data_list_3 = [pd.read_csv(f, sep='\\t', index_col=None, header=0) for f in harvey_tsv_files]\n",
    "harvey_data_df_3 = pd.concat(harvey_data_list_3, axis=0)\n",
    "# remove irrelevant rows \n",
    "harvey_data_df_3.dropna(axis=0,subset=['text'],inplace=True)\n",
    "harvey_data_df_3 = harvey_data_df_3[harvey_data_df_3.loc[:, 'text'].apply(lambda x: tweet_matcher.match(x.lower()) is not None)]\n",
    "# make sure that the columns match\n",
    "harvey_data_df_3 = convert_historical_df_to_standard(harvey_data_df_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine EVERYTHING."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# harvey_data_combined = pd.concat([harvey_data_df_1, harvey_data_df_2, harvey_data_df_3], axis=0)\n",
    "# remove duplicates? we have to be careful not to destroy useful data!\n",
    "print('complete data has %d tweets'%(harvey_data_combined.shape[0]))\n",
    "harvey_data_deduplicated = harvey_data_combined.drop_duplicates('id', inplace=False)\n",
    "print('deduplicated data has %d tweets'%(harvey_data_deduplicated.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('deduplicated data has %d tweets'%(harvey_data_deduplicated.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete data has 176985299 tweets\n"
     ]
    }
   ],
   "source": [
    "print('complete data has %d tweets'%(harvey_data_combined.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to file!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "harvey_data_combined.to_csv('../../data/mined_tweets/harvey_combined.tsv', encoding='utf-8', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do the same thing with the rest of the files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Irma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_archive_tweets(data_dir, keyword):\n",
    "    file_keyword = '#%s'%(keyword)\n",
    "    file_matcher = re.compile('archive.*%s.*.gz'%(file_keyword))\n",
    "    json_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if file_matcher.match(f.lower())]\n",
    "    data_list = []\n",
    "    tweet_matcher = re.compile('.*%s.*'%(keyword))\n",
    "    tweet_ctr = 0\n",
    "    for f in json_files:\n",
    "        for l in gzip.open(f, 'r'):\n",
    "            l_json = literal_eval(l.strip())\n",
    "            if(tweet_matcher.match(l_json['text'].lower())):\n",
    "                l_data = extract_data_from_json(l_json)\n",
    "                data_list.append(l_data)\n",
    "                tweet_ctr += 1\n",
    "                if(len(data_list) % 1000 == 0):\n",
    "                    print('processed %d tweets'%(len(data_list)))\n",
    "    data_df = pd.concat([pd.Series(t) for t in data_list], axis=1).transpose()\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_historical_tweets(data_dir, keyword):\n",
    "    file_matcher = re.compile('(?<!archive_)#.*%s.*.gz'%(keyword))\n",
    "    tweet_matcher = re.compile('.*%s.*'%(keyword))\n",
    "    tsv_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if file_matcher.match(f.lower())]\n",
    "    data_list = [pd.read_csv(f, sep='\\t', index_col=None, header=0) for f in tsv_files]\n",
    "    data_df = pd.concat(data_list, axis=0)\n",
    "    # remove irrelevant rows \n",
    "    data_df.dropna(axis=0,subset=['text'],inplace=True)\n",
    "    data_df = data_df[data_df.loc[:, 'text'].apply(lambda x: tweet_matcher.match(x.lower()) is not None)]\n",
    "    # make sure that the columns match\n",
    "    data_df = convert_historical_df_to_standard(data_df)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1000 tweets\n",
      "processed 2000 tweets\n",
      "processed 3000 tweets\n",
      "processed 4000 tweets\n",
      "processed 5000 tweets\n",
      "processed 6000 tweets\n",
      "processed 7000 tweets\n",
      "processed 8000 tweets\n",
      "processed 9000 tweets\n",
      "processed 10000 tweets\n",
      "processed 11000 tweets\n",
      "processed 12000 tweets\n",
      "processed 13000 tweets\n",
      "processed 14000 tweets\n",
      "processed 15000 tweets\n",
      "processed 16000 tweets\n",
      "processed 17000 tweets\n",
      "processed 18000 tweets\n",
      "processed 19000 tweets\n",
      "processed 20000 tweets\n",
      "processed 21000 tweets\n",
      "processed 22000 tweets\n",
      "processed 23000 tweets\n",
      "processed 24000 tweets\n",
      "processed 25000 tweets\n",
      "processed 26000 tweets\n",
      "processed 27000 tweets\n",
      "processed 28000 tweets\n",
      "processed 29000 tweets\n",
      "processed 30000 tweets\n",
      "processed 31000 tweets\n",
      "processed 32000 tweets\n",
      "processed 33000 tweets\n",
      "processed 34000 tweets\n",
      "processed 35000 tweets\n",
      "processed 36000 tweets\n",
      "processed 37000 tweets\n",
      "processed 38000 tweets\n",
      "processed 39000 tweets\n",
      "processed 40000 tweets\n",
      "processed 41000 tweets\n",
      "processed 42000 tweets\n",
      "processed 43000 tweets\n",
      "processed 44000 tweets\n",
      "processed 45000 tweets\n",
      "processed 46000 tweets\n",
      "processed 47000 tweets\n",
      "processed 48000 tweets\n",
      "processed 49000 tweets\n",
      "processed 50000 tweets\n",
      "processed 51000 tweets\n",
      "processed 52000 tweets\n",
      "processed 53000 tweets\n",
      "processed 54000 tweets\n",
      "processed 55000 tweets\n",
      "processed 56000 tweets\n",
      "processed 57000 tweets\n",
      "processed 58000 tweets\n",
      "processed 59000 tweets\n",
      "processed 60000 tweets\n",
      "processed 61000 tweets\n",
      "processed 62000 tweets\n",
      "processed 63000 tweets\n",
      "processed 64000 tweets\n",
      "processed 65000 tweets\n",
      "processed 66000 tweets\n",
      "processed 67000 tweets\n",
      "processed 68000 tweets\n",
      "processed 69000 tweets\n",
      "processed 70000 tweets\n",
      "processed 71000 tweets\n",
      "processed 72000 tweets\n",
      "processed 73000 tweets\n",
      "processed 74000 tweets\n",
      "processed 75000 tweets\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../../data/mined_tweets/'\n",
    "keyword = 'irma'\n",
    "irma_archive_tweets = load_archive_tweets(data_dir, keyword)\n",
    "irma_historical_tweets = load_historical_tweets(data_dir, keyword)\n",
    "irma_combined = pd.concat([irma_archive_tweets, irma_historical_tweets], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save\n",
    "irma_combined.to_csv('../../data/mined_tweets/irma_combined.tsv', sep='\\t', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_streaming_data(data_dir, keyword):\n",
    "    keyword_matcher = re.compile('.*%s_tweets.*'%(keyword))\n",
    "    data_files = [os.path.join(data_dir, f) for f in os.listdir(data_dir) if keyword_matcher.match(f)]\n",
    "    data_list = []\n",
    "    tweet_matcher = re.compile('.*%s.*'%(keyword))\n",
    "    tweet_ctr = 0\n",
    "    for f in data_files:\n",
    "        for l in gzip.open(f, 'r'):\n",
    "            try:\n",
    "                l_json = json.loads(l.strip())\n",
    "    #             l_json = literal_eval(l.strip())\n",
    "                if(tweet_matcher.match(l_json['text'].lower())):\n",
    "                    l_data = extract_data_from_json(l_json)\n",
    "                    data_list.append(l_data)\n",
    "                    tweet_ctr += 1\n",
    "                    if(len(data_list) % 1000 == 0):\n",
    "                        print('processed %d tweets'%(len(data_list)))\n",
    "            except Exception, e:\n",
    "                print('failed to load tweet %s'%(l))\n",
    "    data_df = pd.concat([pd.Series(t) for t in data_list], axis=1).transpose()\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyword = 'maria'\n",
    "maria_df_1 = load_historical_tweets(data_dir, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1000 tweets\n",
      "processed 2000 tweets\n",
      "processed 3000 tweets\n",
      "processed 4000 tweets\n",
      "processed 5000 tweets\n",
      "processed 6000 tweets\n",
      "processed 7000 tweets\n",
      "processed 8000 tweets\n",
      "processed 9000 tweets\n",
      "processed 10000 tweets\n",
      "processed 11000 tweets\n",
      "processed 12000 tweets\n",
      "processed 13000 tweets\n",
      "processed 14000 tweets\n",
      "processed 15000 tweets\n",
      "processed 16000 tweets\n",
      "processed 17000 tweets\n",
      "processed 18000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":2,\"timestamp_ms\":\"1506481314460\"}}\n",
      "\n",
      "processed 19000 tweets\n",
      "processed 20000 tweets\n",
      "processed 21000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":2,\"timestamp_ms\":\"1506486842228\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":2,\"timestamp_ms\":\"1506486843352\"}}\n",
      "\n",
      "processed 22000 tweets\n",
      "processed 23000 tweets\n",
      "processed 24000 tweets\n",
      "processed 25000 tweets\n",
      "processed 26000 tweets\n",
      "processed 27000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":3,\"timestamp_ms\":\"1506500469026\"}}\n",
      "\n",
      "processed 28000 tweets\n",
      "processed 29000 tweets\n",
      "processed 30000 tweets\n",
      "processed 31000 tweets\n",
      "processed 32000 tweets\n",
      "processed 33000 tweets\n",
      "processed 34000 tweets\n",
      "processed 35000 tweets\n",
      "processed 36000 tweets\n",
      "processed 37000 tweets\n",
      "processed 38000 tweets\n",
      "processed 39000 tweets\n",
      "processed 40000 tweets\n",
      "processed 41000 tweets\n",
      "processed 42000 tweets\n",
      "processed 43000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":2,\"timestamp_ms\":\"1506538432809\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":6,\"timestamp_ms\":\"1506538434440\"}}\n",
      "\n",
      "processed 44000 tweets\n",
      "processed 45000 tweets\n",
      "processed 46000 tweets\n",
      "processed 47000 tweets\n",
      "processed 48000 tweets\n",
      "processed 49000 tweets\n",
      "processed 50000 tweets\n",
      "processed 51000 tweets\n",
      "processed 52000 tweets\n",
      "processed 53000 tweets\n",
      "processed 54000 tweets\n",
      "processed 55000 tweets\n",
      "processed 56000 tweets\n",
      "processed 57000 tweets\n",
      "processed 58000 tweets\n",
      "processed 59000 tweets\n",
      "processed 60000 tweets\n",
      "processed 61000 tweets\n",
      "processed 62000 tweets\n",
      "processed 63000 tweets\n",
      "processed 64000 tweets\n",
      "processed 65000 tweets\n",
      "processed 66000 tweets\n",
      "processed 67000 tweets\n",
      "processed 68000 tweets\n",
      "processed 69000 tweets\n",
      "processed 70000 tweets\n",
      "processed 71000 tweets\n",
      "processed 72000 tweets\n",
      "processed 73000 tweets\n",
      "processed 74000 tweets\n",
      "processed 75000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":12,\"timestamp_ms\":\"1506625808048\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":12,\"timestamp_ms\":\"1506625808789\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":14,\"timestamp_ms\":\"1506625809597\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":12,\"timestamp_ms\":\"1506625814839\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":27,\"timestamp_ms\":\"1506626430854\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":37,\"timestamp_ms\":\"1506626430906\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":29,\"timestamp_ms\":\"1506626430921\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":26,\"timestamp_ms\":\"1506626430982\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":29,\"timestamp_ms\":\"1506626435903\"}}\n",
      "\n",
      "processed 76000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":50,\"timestamp_ms\":\"1506630178022\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":34,\"timestamp_ms\":\"1506630178034\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":39,\"timestamp_ms\":\"1506630178124\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":46,\"timestamp_ms\":\"1506630178170\"}}\n",
      "\n",
      "processed 77000 tweets\n",
      "processed 78000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":43,\"timestamp_ms\":\"1506633499261\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":71,\"timestamp_ms\":\"1506633500559\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":68,\"timestamp_ms\":\"1506633501371\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":55,\"timestamp_ms\":\"1506633501683\"}}\n",
      "\n",
      "processed 79000 tweets\n",
      "processed 80000 tweets\n",
      "processed 81000 tweets\n",
      "processed 82000 tweets\n",
      "processed 83000 tweets\n",
      "processed 84000 tweets\n",
      "processed 85000 tweets\n",
      "processed 86000 tweets\n",
      "processed 87000 tweets\n",
      "processed 88000 tweets\n",
      "processed 89000 tweets\n",
      "processed 90000 tweets\n",
      "processed 91000 tweets\n",
      "processed 92000 tweets\n",
      "processed 93000 tweets\n",
      "processed 94000 tweets\n",
      "processed 95000 tweets\n",
      "processed 96000 tweets\n",
      "processed 97000 tweets\n",
      "processed 98000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":3,\"timestamp_ms\":\"1506735718844\"}}\n",
      "\n",
      "processed 99000 tweets\n",
      "processed 100000 tweets\n",
      "processed 101000 tweets\n",
      "processed 102000 tweets\n",
      "processed 103000 tweets\n",
      "processed 104000 tweets\n",
      "processed 105000 tweets\n",
      "processed 106000 tweets\n",
      "processed 107000 tweets\n",
      "processed 108000 tweets\n",
      "processed 109000 tweets\n",
      "processed 110000 tweets\n",
      "processed 111000 tweets\n",
      "processed 112000 tweets\n",
      "processed 113000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":6,\"timestamp_ms\":\"1506781318815\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":12,\"timestamp_ms\":\"1506781318868\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":11,\"timestamp_ms\":\"1506781318889\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":8,\"timestamp_ms\":\"1506781318890\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":20,\"timestamp_ms\":\"1506781319844\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":13,\"timestamp_ms\":\"1506781319846\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":31,\"timestamp_ms\":\"1506781319866\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":25,\"timestamp_ms\":\"1506781319999\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":17,\"timestamp_ms\":\"1506781320843\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":29,\"timestamp_ms\":\"1506781320916\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":33,\"timestamp_ms\":\"1506781320919\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":35,\"timestamp_ms\":\"1506781320992\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":30,\"timestamp_ms\":\"1506781321959\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":21,\"timestamp_ms\":\"1506781322057\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":38,\"timestamp_ms\":\"1506781322178\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":38,\"timestamp_ms\":\"1506781322232\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":31,\"timestamp_ms\":\"1506781322959\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":33,\"timestamp_ms\":\"1506781324037\"}}\n",
      "\n",
      "processed 114000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":30,\"timestamp_ms\":\"1506781676139\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":37,\"timestamp_ms\":\"1506781676194\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":51,\"timestamp_ms\":\"1506781676341\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":48,\"timestamp_ms\":\"1506781676796\"}}\n",
      "\n",
      "processed 115000 tweets\n",
      "processed 116000 tweets\n",
      "processed 117000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":49,\"timestamp_ms\":\"1506786129478\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":52,\"timestamp_ms\":\"1506786139061\"}}\n",
      "\n",
      "processed 118000 tweets\n",
      "processed 119000 tweets\n",
      "processed 120000 tweets\n",
      "processed 121000 tweets\n",
      "processed 122000 tweets\n",
      "processed 123000 tweets\n",
      "processed 124000 tweets\n",
      "processed 125000 tweets\n",
      "processed 126000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":53,\"timestamp_ms\":\"1506801676932\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":40,\"timestamp_ms\":\"1506801677276\"}}\n",
      "\n",
      "processed 127000 tweets\n",
      "processed 128000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":33,\"timestamp_ms\":\"1506806339272\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":57,\"timestamp_ms\":\"1506806339340\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":43,\"timestamp_ms\":\"1506806339350\"}}\n",
      "\n",
      "processed 129000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":43,\"timestamp_ms\":\"1506807471223\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":50,\"timestamp_ms\":\"1506807471997\"}}\n",
      "\n",
      "processed 130000 tweets\n",
      "processed 131000 tweets\n",
      "processed 132000 tweets\n",
      "processed 133000 tweets\n",
      "processed 134000 tweets\n",
      "processed 135000 tweets\n",
      "processed 136000 tweets\n",
      "processed 137000 tweets\n",
      "processed 138000 tweets\n",
      "processed 139000 tweets\n",
      "processed 140000 tweets\n",
      "processed 141000 tweets\n",
      "processed 142000 tweets\n",
      "processed 143000 tweets\n",
      "processed 144000 tweets\n",
      "processed 145000 tweets\n",
      "processed 146000 tweets\n",
      "processed 147000 tweets\n",
      "processed 148000 tweets\n",
      "processed 149000 tweets\n",
      "processed 150000 tweets\n",
      "processed 151000 tweets\n",
      "processed 152000 tweets\n",
      "processed 153000 tweets\n",
      "processed 154000 tweets\n",
      "processed 155000 tweets\n",
      "processed 156000 tweets\n",
      "processed 157000 tweets\n",
      "processed 158000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":58,\"timestamp_ms\":\"1506891319045\"}}\n",
      "\n",
      "processed 159000 tweets\n",
      "processed 160000 tweets\n",
      "processed 161000 tweets\n",
      "processed 162000 tweets\n",
      "processed 163000 tweets\n",
      "processed 164000 tweets\n",
      "processed 165000 tweets\n",
      "processed 166000 tweets\n",
      "processed 167000 tweets\n",
      "processed 168000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":13,\"timestamp_ms\":\"1507811629441\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":7,\"timestamp_ms\":\"1507811630161\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":9,\"timestamp_ms\":\"1507811631204\"}}\n",
      "\n",
      "processed 169000 tweets\n",
      "processed 170000 tweets\n",
      "processed 171000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":9,\"timestamp_ms\":\"1507838741400\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":17,\"timestamp_ms\":\"1507838743945\"}}\n",
      "\n",
      "processed 172000 tweets\n",
      "processed 173000 tweets\n",
      "processed 174000 tweets\n",
      "processed 175000 tweets\n",
      "processed 176000 tweets\n",
      "processed 177000 tweets\n",
      "processed 178000 tweets\n",
      "processed 179000 tweets\n",
      "processed 180000 tweets\n",
      "processed 181000 tweets\n",
      "processed 182000 tweets\n",
      "processed 183000 tweets\n",
      "processed 184000 tweets\n",
      "processed 185000 tweets\n",
      "processed 186000 tweets\n",
      "processed 187000 tweets\n",
      "processed 188000 tweets\n",
      "processed 189000 tweets\n",
      "processed 190000 tweets\n",
      "processed 191000 tweets\n",
      "failed to load tweet {\"limit\":{\"track\":4,\"timestamp_ms\":\"1507734553177\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":2,\"timestamp_ms\":\"1507734554630\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":12,\"timestamp_ms\":\"1507734561240\"}}\n",
      "\n",
      "failed to load tweet {\"limit\":{\"track\":3,\"timestamp_ms\":\"1507734564115\"}}\n",
      "\n",
      "processed 192000 tweets\n",
      "processed 193000 tweets\n",
      "processed 194000 tweets\n",
      "processed 195000 tweets\n"
     ]
    }
   ],
   "source": [
    "maria_df_2 = load_streaming_data(data_dir, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maria_combined = pd.concat([maria_df_1, maria_df_2], axis=0)\n",
    "maria_combined.to_csv('../../data/mined_tweets/maria_combined.tsv', sep='\\t', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "keyword = 'nate'\n",
    "nate_df_1 = load_historical_tweets(data_dir, keyword)\n",
    "nate_df_1.to_csv('../../data/mined_tweets/nate_combined.tsv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ophelia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 1000 tweets\n",
      "processed 2000 tweets\n",
      "processed 3000 tweets\n",
      "processed 4000 tweets\n",
      "processed 5000 tweets\n",
      "processed 6000 tweets\n",
      "processed 7000 tweets\n",
      "processed 8000 tweets\n",
      "processed 9000 tweets\n"
     ]
    }
   ],
   "source": [
    "keyword = 'ophelia'\n",
    "ophelia_df_1 = load_historical_tweets(data_dir, keyword)\n",
    "ophelia_df_2 = load_streaming_data(data_dir, keyword)\n",
    "ophelia_df_combined = pd.concat([ophelia_df_1, ophelia_df_2], axis=0)\n",
    "ophelia_df_combined.to_csv('../../data/mined_tweets/ophelia_combined.tsv', sep='\\t', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
