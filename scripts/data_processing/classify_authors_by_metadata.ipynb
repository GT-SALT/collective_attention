{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify authors by metadata\n",
    "Let's try to expand the coverage of local vs. non-local authors using the raw posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import dateutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310514\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>verified</th>\n",
       "      <th>date</th>\n",
       "      <th>data_name_fixed</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Xavier Mitjavila Moix</td>\n",
       "      <td>1024378674651574272</td>\n",
       "      <td>barcelona</td>\n",
       "      <td>Xavier Mitjavila Moix de París, Francia y Barc...</td>\n",
       "      <td>163</td>\n",
       "      <td>43</td>\n",
       "      <td>18485</td>\n",
       "      <td>Tue Jul 31 19:38:02 +0000 2018</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>florence</td>\n",
       "      <td>xaviermitjavila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Muhemmed Asfand Yar</td>\n",
       "      <td>48365860</td>\n",
       "      <td>New York, USA</td>\n",
       "      <td>Google Expert #DigitalMarketing #SEO #SEM #Soc...</td>\n",
       "      <td>602</td>\n",
       "      <td>399</td>\n",
       "      <td>131224</td>\n",
       "      <td>Thu Jun 18 14:30:14 +0000 2009</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>florence</td>\n",
       "      <td>muhemmedasfand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Snowbie</td>\n",
       "      <td>730825682</td>\n",
       "      <td>Dublin City, Ireland</td>\n",
       "      <td>Forecaster for http://Metcast.net, senior mode...</td>\n",
       "      <td>992</td>\n",
       "      <td>1140</td>\n",
       "      <td>20599</td>\n",
       "      <td>Wed Aug 01 14:52:44 +0000 2012</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>florence</td>\n",
       "      <td>bruensryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Deplorable Diana +++</td>\n",
       "      <td>19195060</td>\n",
       "      <td>I Block Trolls USA</td>\n",
       "      <td>This is a PC FREE Zone! There is a possibility...</td>\n",
       "      <td>2637</td>\n",
       "      <td>2975</td>\n",
       "      <td>58774</td>\n",
       "      <td>Mon Jan 19 18:38:04 +0000 2009</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>florence</td>\n",
       "      <td>fancygap1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>William E. Lewis, Jr</td>\n",
       "      <td>29753150</td>\n",
       "      <td>Fort Lauderdale, Florida</td>\n",
       "      <td>Former AM radio talk show host @AM740WSBR @AM1...</td>\n",
       "      <td>12458</td>\n",
       "      <td>10385</td>\n",
       "      <td>274683</td>\n",
       "      <td>Wed Apr 08 16:24:57 +0000 2009</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>florence</td>\n",
       "      <td>4billlewis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                   id                  location  \\\n",
       "0  Xavier Mitjavila Moix  1024378674651574272                 barcelona   \n",
       "1    Muhemmed Asfand Yar             48365860             New York, USA   \n",
       "2                Snowbie            730825682      Dublin City, Ireland   \n",
       "3   Deplorable Diana +++             19195060        I Block Trolls USA   \n",
       "4   William E. Lewis, Jr             29753150  Fort Lauderdale, Florida   \n",
       "\n",
       "                                         description  followers_count  \\\n",
       "0  Xavier Mitjavila Moix de París, Francia y Barc...              163   \n",
       "1  Google Expert #DigitalMarketing #SEO #SEM #Soc...              602   \n",
       "2  Forecaster for http://Metcast.net, senior mode...              992   \n",
       "3  This is a PC FREE Zone! There is a possibility...             2637   \n",
       "4  Former AM radio talk show host @AM740WSBR @AM1...            12458   \n",
       "\n",
       "   friends_count  statuses_count                      created_at  verified  \\\n",
       "0             43           18485  Tue Jul 31 19:38:02 +0000 2018     False   \n",
       "1            399          131224  Thu Jun 18 14:30:14 +0000 2009     False   \n",
       "2           1140           20599  Wed Aug 01 14:52:44 +0000 2012     False   \n",
       "3           2975           58774  Mon Jan 19 18:38:04 +0000 2009     False   \n",
       "4          10385          274683  Wed Apr 08 16:24:57 +0000 2009     False   \n",
       "\n",
       "        date data_name_fixed         username  \n",
       "0 2018-09-03        florence  xaviermitjavila  \n",
       "1 2018-09-03        florence   muhemmedasfand  \n",
       "2 2018-09-03        florence       bruensryan  \n",
       "3 2018-09-03        florence        fancygap1  \n",
       "4 2018-09-03        florence       4billlewis  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57071\n"
     ]
    }
   ],
   "source": [
    "user_data = pd.read_csv('../../data/mined_tweets/tweet_user_data/user_data_archive.gz', sep='\\t', index_col=False, compression='gzip', converters={'date' : dateutil.parser.parse})\n",
    "user_data.fillna('', inplace=True)\n",
    "print(user_data.shape[0])\n",
    "display(user_data.head())\n",
    "author_var = 'username'\n",
    "data_name_var = 'data_name_fixed'\n",
    "date_var = 'date'\n",
    "user_meta_vars = ['location', 'description', 'followers_count', 'friends_count', 'created_at']\n",
    "user_data.sort_values([author_var, date_var], inplace=True, ascending=True)\n",
    "user_dedup = user_data.drop_duplicates([author_var, data_name_var]).loc[:, user_meta_vars + [author_var, data_name_var]]\n",
    "print(user_dedup.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "## load full data\n",
    "author_var = 'username'\n",
    "data_name_var = 'data_name_fixed'\n",
    "txt_var = 'txt'\n",
    "NE_list_var = 'NE_list'\n",
    "usecols = [author_var, data_name_var, txt_var, NE_list_var]\n",
    "full_data = pd.read_csv('../../data/mined_tweets/combined_tweet_tag_data.gz', sep='\\t', index_col=False, compression='gzip', converters={'NE_list' : literal_eval}, usecols=usecols)\n",
    "## restrict NE list to tokens\n",
    "full_data = full_data.assign(**{\n",
    "    'NE_list_tokens' : full_data.loc[:, NE_list_var].apply(lambda x: [y[0] for y in x])\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                      []\n",
       "1                      []\n",
       "2      [POTUS, Hurricane]\n",
       "3                      []\n",
       "4                      []\n",
       "5                      []\n",
       "6                      []\n",
       "7         [Trump, island]\n",
       "8    [Op_RUMAN, Dominica]\n",
       "9                      []\n",
       "Name: NE_list_tokens, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data.loc[:, 'NE_list_tokens'].iloc[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's mark local/non-local authors based on `location` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?<=[, ])fl$|^fl,\\s+|(?<=[, ])nc$|^nc,\\s+|(?<=[, ])sc$|^sc,\\s+|(?<=[, ])va$|^va,\\s+|(?<=[, ])ga$|^ga,\\s+|(?<=[, ])florida$|^florida,\\s+|(?<=[, ])north carolina$|^north carolina,\\s+|(?<=[, ])south carolina$|^south carolina,\\s+|(?<=[, ])virginia$|^virginia,\\s+|(?<=[, ])georgia$|^georgia,\\s+\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from data_helpers import DATA_NAME_STATES_LOOKUP\n",
    "DATA_NAME_STATES_MATCHER = {\n",
    "    k : re.compile('|'.join(['(?<=[, ])%s$|^%s,\\s+'%((loc.lower(),)*2) for loc in v]))\n",
    "    for k,v in DATA_NAME_STATES_LOOKUP.items()\n",
    "}\n",
    "print(DATA_NAME_STATES_MATCHER['florence'].pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_var = 'location'\n",
    "local_var = 'is_local'\n",
    "user_dedup = user_dedup.assign(**{\n",
    "    local_var : user_dedup.apply(lambda x: DATA_NAME_STATES_MATCHER[x.loc[data_name_var]].search(x.loc[loc_var].lower()) is not None, axis=1)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12066/57071 local authors\n"
     ]
    }
   ],
   "source": [
    "print('%d/%d local authors'%(user_dedup.loc[:, local_var].sum(), user_dedup.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12178/1353597 authors represented\n"
     ]
    }
   ],
   "source": [
    "full_user_data = pd.merge(full_data, user_dedup.loc[:, [author_var, data_name_var, local_var]], on=[author_var, data_name_var], how='inner')\n",
    "print('%d/%d authors represented'%(full_user_data.loc[:, author_var].nunique(), full_data.loc[:, author_var].nunique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test local/non-local classification\n",
    "Let's start off with a basic BOW classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12178, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,3), max_features=5000, lowercase=True, stop_words='english')\n",
    "# group text by author: one doc per author\n",
    "author_txt = full_user_data.groupby(author_var).apply(lambda x: ' '.join(x.loc[:, 'txt'].values))\n",
    "author_labels = np.array([x for x in full_user_data.groupby(author_var).apply(lambda x: x.loc[:, local_var].iloc[0])]).astype(int)\n",
    "author_txt_dtm = cv.fit_transform(author_txt)\n",
    "print(author_txt_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "url                  58014\n",
       "irma                 28478\n",
       "hurricaneirma        14913\n",
       "puertorico           13156\n",
       "hurricane            10201\n",
       "florida               9392\n",
       "rt                    9288\n",
       "harvey                6812\n",
       "irma url              5488\n",
       "hurricane irma        4556\n",
       "storm                 4497\n",
       "maria                 4405\n",
       "help                  4049\n",
       "miami                 3935\n",
       "hurricaneirma url     3665\n",
       "power                 3576\n",
       "hurricanemaria        3502\n",
       "people                3302\n",
       "trump                 3261\n",
       "hurricanemichael      3111\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## most frequent features\n",
    "author_txt_dtm_sum = pd.Series(np.squeeze(np.array(author_txt_dtm.sum(axis=0))), index=sorted(cv.vocabulary_, key=lambda x: cv.vocabulary_[x]))\n",
    "author_txt_dtm_sum.sort_values(ascending=False, inplace=True)\n",
    "display(author_txt_dtm_sum.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## balance data\n",
    "logit = LogisticRegression(class_weight='balanced', solver='lbfgs')\n",
    "## cross-validation\n",
    "cross_val_folds = 10\n",
    "cross_val_results = cross_validate(logit, author_txt_dtm, author_labels, cv=cross_val_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(cross_val_results['test_score'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall we get a pretty low accuracy. Not good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we'll have better luck with the NEs that authors use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12178, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(ngram_range=(1,1), max_features=5000, lowercase=True, stop_words='english')\n",
    "# group text by author: one doc per author\n",
    "NE_token_var = 'NE_list_tokens'\n",
    "author_NE_txt = full_user_data.groupby(author_var).apply(lambda x: ' '.join([z for y in x.loc[:, NE_token_var] for z in y]))\n",
    "author_labels = np.array([x for x in full_user_data.groupby(author_var).apply(lambda x: x.loc[:, local_var].iloc[0])]).astype(int)\n",
    "author_NE_txt_dtm = cv.fit_transform(author_NE_txt)\n",
    "print(author_NE_txt_dtm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "florida           4752\n",
       "miami             2261\n",
       "irma              1823\n",
       "puerto_rico       1803\n",
       "houston           1658\n",
       "san_juan          1433\n",
       "trump             1379\n",
       "fl                1229\n",
       "naples            1086\n",
       "huracán            949\n",
       "tampa              807\n",
       "texas              795\n",
       "hurricane          784\n",
       "harvey             715\n",
       "georgia            639\n",
       "north_carolina     593\n",
       "hurricane_irma     576\n",
       "florida_keys       544\n",
       "florence           525\n",
       "nc                 510\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## most frequauthor_NE_txt_dtm_sument features\n",
    "author_NE_txt_dtm_sum = pd.Series(np.squeeze(np.array(author_NE_txt_dtm.sum(axis=0))), index=sorted(cv.vocabulary_, key=lambda x: cv.vocabulary_[x]))\n",
    "author_NE_txt_dtm_sum.sort_values(ascending=False, inplace=True)\n",
    "display(author_NE_txt_dtm_sum.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "## balance data\n",
    "logit = LogisticRegression(class_weight='balanced', solver='lbfgs')\n",
    "## cross-validation\n",
    "cross_val_folds = 10\n",
    "cross_val_results = cross_validate(logit, author_NE_txt_dtm, author_labels, cv=cross_val_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.boxplot(cross_val_results['test_score'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow this is worse. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify local vs. non-local per-dataset\n",
    "Wait a second. We should be modeling the conditional probability of local authorship based on the data. e\n",
    "\n",
    "Example: $P(\\text{local} | \\text{San Juan}, \\text{Maria}) \\neq P(\\text{local} | \\text{San Juan}, \\text{Harvey})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a separate classifier for each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data=florence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train score = 0.779 +/- 0.002\n",
      "mean test score = 0.685 +/- 0.016\n",
      "data=harvey\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train score = 0.684 +/- 0.001\n",
      "mean test score = 0.629 +/- 0.008\n",
      "data=irma\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:758: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train score = 0.768 +/- 0.001\n",
      "mean test score = 0.723 +/- 0.005\n",
      "data=maria\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean train score = 0.896 +/- 0.004\n",
      "mean test score = 0.857 +/- 0.010\n",
      "data=michael\n",
      "mean train score = 0.803 +/- 0.002\n",
      "mean test score = 0.673 +/- 0.011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/sklearn/utils/deprecation.py:125: FutureWarning: You are accessing a training score ('train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "def fit_test_model(data, author_var='username', txt_var='txt', dep_var='is_local', cross_val_folds=10, max_features=5000, ngram_range=(1,1), reg_weight_C=0.01):\n",
    "    cv = CountVectorizer(ngram_range=(1,3), max_features=max_features, lowercase=True, stop_words='english')\n",
    "    # group text by author: one doc per author\n",
    "    author_txt = data.groupby(author_var).apply(lambda x: ' '.join(x.loc[:, txt_var].values))\n",
    "    author_labels = np.array([x for x in data.groupby(author_var).apply(lambda x: x.loc[:, dep_var].iloc[0])]).astype(int)\n",
    "    author_txt_dtm = cv.fit_transform(author_txt)\n",
    "    ## balance data\n",
    "    logit = LogisticRegression(class_weight='balanced', solver='lbfgs', C=reg_weight_C)\n",
    "    ## cross-validation\n",
    "    cross_val_results = cross_validate(logit, author_txt_dtm, author_labels, cv=cross_val_folds)\n",
    "    return cross_val_results\n",
    "    \n",
    "data_name_var = 'data_name_fixed'\n",
    "cross_val_folds = 10\n",
    "txt_var = 'txt'\n",
    "ngram_range = (1,1)\n",
    "max_features = 1000\n",
    "for data_name_i, data_i in full_user_data.groupby(data_name_var):\n",
    "    print('data=%s'%(data_name_i))\n",
    "    results_i = fit_test_model(data_i, txt_var=txt_var, cross_val_folds=cross_val_folds, ngram_range=ngram_range, max_features=max_features)\n",
    "    train_mean_i = np.mean(results_i['train_score'])\n",
    "    test_mean_i = np.mean(results_i['test_score'])\n",
    "    train_sd_i = np.std(results_i['train_score']) / (cross_val_folds**.5)\n",
    "    test_sd_i = np.std(results_i['test_score']) / (cross_val_folds**.5)\n",
    "    print('mean train score = %.3f +/- %.3f'%(train_mean_i, train_sd_i))\n",
    "    print('mean test score = %.3f +/- %.3f'%(test_mean_i, test_sd_i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify authors by media account vs. personal account\n",
    "Can we determine whether an author is posting from a media account versus a personal account? Some say [yes](https://www.aclweb.org/anthology/W16-3904)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to label the accounts manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n"
     ]
    }
   ],
   "source": [
    "label_cols = ['username', 'description', 'followers_count', 'data_name_fixed']\n",
    "label_file = '../../data/mined_tweets/tweet_user_data/user_data_media_label.tsv'\n",
    "# get even sample from all datasets\n",
    "data_name_var = 'data_name_fixed'\n",
    "np.random.seed(123)\n",
    "sample_size = 100\n",
    "# sample_size = 500 # TODO: ambitious goal => build separate classifier with labelled data\n",
    "user_label_data = user_dedup.groupby(data_name_var).apply(lambda x: x.loc[np.random.choice(x.index, sample_size, replace=False), :]).reset_index(drop=True)\n",
    "print(user_label_data.shape[0])\n",
    "user_label_data.to_csv(label_file, columns=label_cols, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/500 organizations\n"
     ]
    }
   ],
   "source": [
    "# after labelling\n",
    "user_label_data_clean = pd.read_csv('../../data/mined_tweets/tweet_user_data/user_data_media_label_clean.tsv', sep='\\t', index_col=False)\n",
    "user_label_data_clean.fillna('', inplace=True)\n",
    "org_var = 'organization'\n",
    "print('%d/%d organizations'%(user_label_data_clean.loc[:, org_var].sum(), user_label_data_clean.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a bit small for classifying, but we can at least which words are more correlated with organizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/ipykernel_launcher.py:15: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  from ipykernel import kernelapp as app\n",
      "/hg190/istewart6/miniconda3/envs/crisis_language/lib/python3.6/site-packages/ipykernel_launcher.py:16: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fox                      5.0\n",
       "site                     5.0\n",
       "group                    5.0\n",
       "updates                  5.0\n",
       "accuweather              4.0\n",
       "online                   4.0\n",
       "company                  4.0\n",
       "official twitter         4.0\n",
       "feed                     4.0\n",
       "official twitter feed    4.0\n",
       "page                     4.0\n",
       "fm                       4.0\n",
       "health                   4.0\n",
       "shrimp                   4.0\n",
       "boxeo                    4.0\n",
       "latest news              4.0\n",
       "wine                     4.0\n",
       "twitter feed             4.0\n",
       "latest                   3.5\n",
       "emergency                3.5\n",
       "dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def get_word_counts(txt):\n",
    "    cv = CountVectorizer(ngram_range=(1,3), min_df=0., stop_words='english')\n",
    "    dtm = cv.fit_transform(txt)\n",
    "    word_counts = np.squeeze(np.array(dtm.sum(axis=0)))\n",
    "    word_counts = pd.Series(word_counts, index=sorted(cv.vocabulary_, key=lambda x: cv.vocabulary_[x]))\n",
    "    return word_counts\n",
    "description_var = 'description'\n",
    "org_txt = user_label_data_clean[user_label_data_clean.loc[:, org_var]==1].loc[:, description_var].values\n",
    "non_org_txt = user_label_data_clean[user_label_data_clean.loc[:, org_var]==0].loc[:, description_var].values\n",
    "org_word_counts = get_word_counts(org_txt)\n",
    "non_org_word_counts = get_word_counts(non_org_txt)\n",
    "# add null vocab, smooth\n",
    "vocab = org_word_counts.index | non_org_word_counts.index\n",
    "org_word_counts = org_word_counts.loc[vocab].fillna(0, inplace=False) + 1.\n",
    "non_org_word_counts = non_org_word_counts.loc[vocab].fillna(0, inplace=False) + 1.\n",
    "org_odds = org_word_counts / non_org_word_counts\n",
    "org_odds.sort_values(inplace=True, ascending=False)\n",
    "display(org_odds.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No surprises! \"Official\" and \"emergency\" seem to be organization-specific words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect organizations with pretrained model\n",
    "Let's cut the line and use [this code](https://bitbucket.org/mdredze/demographer/src/peoples2018/) to detect organization.\n",
    "\n",
    "```\n",
    "git clone https://bitbucket.org/mdredze/demographer.git --branch peoples2018\n",
    "cd demographer\n",
    "python setup.py install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310514\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>description</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>friends_count</th>\n",
       "      <th>statuses_count</th>\n",
       "      <th>created_at</th>\n",
       "      <th>verified</th>\n",
       "      <th>date</th>\n",
       "      <th>data_name_fixed</th>\n",
       "      <th>username</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Xavier Mitjavila Moix</td>\n",
       "      <td>1024378674651574272</td>\n",
       "      <td>barcelona</td>\n",
       "      <td>Xavier Mitjavila Moix de París, Francia y Barc...</td>\n",
       "      <td>163</td>\n",
       "      <td>43</td>\n",
       "      <td>18485</td>\n",
       "      <td>Tue Jul 31 19:38:02 +0000 2018</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>florence</td>\n",
       "      <td>xaviermitjavila</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Muhemmed Asfand Yar</td>\n",
       "      <td>48365860</td>\n",
       "      <td>New York, USA</td>\n",
       "      <td>Google Expert #DigitalMarketing #SEO #SEM #Soc...</td>\n",
       "      <td>602</td>\n",
       "      <td>399</td>\n",
       "      <td>131224</td>\n",
       "      <td>Thu Jun 18 14:30:14 +0000 2009</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>florence</td>\n",
       "      <td>muhemmedasfand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Snowbie</td>\n",
       "      <td>730825682</td>\n",
       "      <td>Dublin City, Ireland</td>\n",
       "      <td>Forecaster for http://Metcast.net, senior mode...</td>\n",
       "      <td>992</td>\n",
       "      <td>1140</td>\n",
       "      <td>20599</td>\n",
       "      <td>Wed Aug 01 14:52:44 +0000 2012</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>florence</td>\n",
       "      <td>bruensryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deplorable Diana +++</td>\n",
       "      <td>19195060</td>\n",
       "      <td>I Block Trolls USA</td>\n",
       "      <td>This is a PC FREE Zone! There is a possibility...</td>\n",
       "      <td>2637</td>\n",
       "      <td>2975</td>\n",
       "      <td>58774</td>\n",
       "      <td>Mon Jan 19 18:38:04 +0000 2009</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>florence</td>\n",
       "      <td>fancygap1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>William E. Lewis, Jr</td>\n",
       "      <td>29753150</td>\n",
       "      <td>Fort Lauderdale, Florida</td>\n",
       "      <td>Former AM radio talk show host @AM740WSBR @AM1...</td>\n",
       "      <td>12458</td>\n",
       "      <td>10385</td>\n",
       "      <td>274683</td>\n",
       "      <td>Wed Apr 08 16:24:57 +0000 2009</td>\n",
       "      <td>False</td>\n",
       "      <td>2018-09-03</td>\n",
       "      <td>florence</td>\n",
       "      <td>4billlewis</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name                   id                  location  \\\n",
       "0  Xavier Mitjavila Moix  1024378674651574272                 barcelona   \n",
       "1    Muhemmed Asfand Yar             48365860             New York, USA   \n",
       "2                Snowbie            730825682      Dublin City, Ireland   \n",
       "3   Deplorable Diana +++             19195060        I Block Trolls USA   \n",
       "4   William E. Lewis, Jr             29753150  Fort Lauderdale, Florida   \n",
       "\n",
       "                                         description  followers_count  \\\n",
       "0  Xavier Mitjavila Moix de París, Francia y Barc...              163   \n",
       "1  Google Expert #DigitalMarketing #SEO #SEM #Soc...              602   \n",
       "2  Forecaster for http://Metcast.net, senior mode...              992   \n",
       "3  This is a PC FREE Zone! There is a possibility...             2637   \n",
       "4  Former AM radio talk show host @AM740WSBR @AM1...            12458   \n",
       "\n",
       "   friends_count  statuses_count                      created_at  verified  \\\n",
       "0             43           18485  Tue Jul 31 19:38:02 +0000 2018     False   \n",
       "1            399          131224  Thu Jun 18 14:30:14 +0000 2009     False   \n",
       "2           1140           20599  Wed Aug 01 14:52:44 +0000 2012     False   \n",
       "3           2975           58774  Mon Jan 19 18:38:04 +0000 2009     False   \n",
       "4          10385          274683  Wed Apr 08 16:24:57 +0000 2009     False   \n",
       "\n",
       "        date data_name_fixed         username  \n",
       "0 2018-09-03        florence  xaviermitjavila  \n",
       "1 2018-09-03        florence   muhemmedasfand  \n",
       "2 2018-09-03        florence       bruensryan  \n",
       "3 2018-09-03        florence        fancygap1  \n",
       "4 2018-09-03        florence       4billlewis  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57071\n"
     ]
    }
   ],
   "source": [
    "user_data = pd.read_csv('../../data/mined_tweets/tweet_user_data/user_data.gz', sep='\\t', index_col=False, compression='gzip', converters={'date' : dateutil.parser.parse})\n",
    "user_data.fillna('', inplace=True)\n",
    "print(user_data.shape[0])\n",
    "display(user_data.head())\n",
    "author_var = 'username'\n",
    "data_name_var = 'data_name_fixed'\n",
    "date_var = 'date'\n",
    "user_meta_vars = ['location', 'description', 'followers_count', 'friends_count', 'created_at', 'statuses_count', 'verified', 'name']\n",
    "user_data.sort_values([author_var, date_var], inplace=True, ascending=True)\n",
    "user_dedup = user_data.drop_duplicates([author_var, data_name_var]).loc[:, user_meta_vars + [author_var, data_name_var]]\n",
    "print(user_dedup.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert to JSON\n",
    "org_detect_cols = ['location', 'description', 'username', 'friends_count', 'followers_count', 'created_at', 'statuses_count', 'verified', 'name']\n",
    "org_detect_null_cols = ['listed_count']\n",
    "author_var = 'username'\n",
    "org_detect_data = user_dedup.drop_duplicates(author_var, inplace=False).loc[:, org_detect_cols].rename(columns={author_var : 'screen_name'})\n",
    "# give 0 for missing data\n",
    "org_detect_data = org_detect_data.assign(**{k : 0. for k in org_detect_null_cols})\n",
    "# org_detect_data = org_detect_data.assign(**{'name' : org_detect_data.loc[:, 'screen_name'].apply(lambda x: x.replace('_', ' '))})\n",
    "org_detect_data_json = org_detect_data.apply(lambda x: x.to_json(), axis=1).values\n",
    "org_detect_input_file = 'demographer/data/test_org_detect_tweets.txt'\n",
    "with open(org_detect_input_file, 'w') as org_detect_input:\n",
    "    org_detect_input.write('\\n'.join(org_detect_data_json))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following command in-line: \n",
    "```\n",
    "cd demographer/\n",
    "python -m demographer.cli.process_tweets --classifier organization --model simple --input data/test_org_detect_tweets.txt --output data/test_org_detect_tweets_labelled.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>label</th>\n",
       "      <th>label_score</th>\n",
       "      <th>organization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>007puk</td>\n",
       "      <td>ind</td>\n",
       "      <td>-3.265107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00titulares</td>\n",
       "      <td>ind</td>\n",
       "      <td>-1.065407</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0111kek0111</td>\n",
       "      <td>ind</td>\n",
       "      <td>-1.723073</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>02cents0</td>\n",
       "      <td>ind</td>\n",
       "      <td>-2.053307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>03steve15</td>\n",
       "      <td>ind</td>\n",
       "      <td>-4.604235</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      username label  label_score  organization\n",
       "0       007puk   ind    -3.265107             0\n",
       "1  00titulares   ind    -1.065407             0\n",
       "2  0111kek0111   ind    -1.723073             0\n",
       "3     02cents0   ind    -2.053307             0\n",
       "4    03steve15   ind    -4.604235             0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ind    46413\n",
      "org     4568\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "org_detect_output_file = 'demographer/data/test_org_detect_tweets_labelled.txt'\n",
    "org_detect_output = [json.loads(x) for x in open(org_detect_output_file, 'r')]\n",
    "# convert to dataframe\n",
    "label_cols = ['screen_name']\n",
    "org_data_label = pd.DataFrame([[x[c] for c in label_cols] + [x['demographics']['indorg']['value'], x['demographics']['indorg']['scores'][x['demographics']['indorg']['value']]] for x in org_detect_output])\n",
    "org_data_label.columns = label_cols + ['label', 'label_score']\n",
    "org_data_label = org_data_label.assign(**{'organization' : (org_data_label.loc[:, 'label']=='org').astype(int)})\n",
    "org_data_label = org_data_label.rename(columns={'screen_name': 'username'})\n",
    "display(org_data_label.head())\n",
    "print(org_data_label.loc[:, 'label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>username</th>\n",
       "      <th>data_name_fixed</th>\n",
       "      <th>description</th>\n",
       "      <th>followers_count</th>\n",
       "      <th>organization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>_angieweems</td>\n",
       "      <td>florence</td>\n",
       "      <td>God,Family, Boxer Dog Lover,Greene County Sher...</td>\n",
       "      <td>828</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>shaunajwv</td>\n",
       "      <td>florence</td>\n",
       "      <td>Reporter, @WVMetroNews Radio Network; Co-Ancho...</td>\n",
       "      <td>2252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>carolinapetes</td>\n",
       "      <td>florence</td>\n",
       "      <td>Christian, 🏌️Golf Life🏌️‍♀️#PGA ***Follow us i...</td>\n",
       "      <td>1628</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gregfordnc</td>\n",
       "      <td>florence</td>\n",
       "      <td>Husband, Dad, Wake County Commissioner. Former...</td>\n",
       "      <td>617</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cryptokells</td>\n",
       "      <td>florence</td>\n",
       "      <td>Living the Colorado Life! + Crypto Enthusiast!...</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        username data_name_fixed  \\\n",
       "0    _angieweems        florence   \n",
       "1      shaunajwv        florence   \n",
       "2  carolinapetes        florence   \n",
       "3     gregfordnc        florence   \n",
       "4    cryptokells        florence   \n",
       "\n",
       "                                         description  followers_count  \\\n",
       "0  God,Family, Boxer Dog Lover,Greene County Sher...              828   \n",
       "1  Reporter, @WVMetroNews Radio Network; Co-Ancho...             2252   \n",
       "2  Christian, 🏌️Golf Life🏌️‍♀️#PGA ***Follow us i...             1628   \n",
       "3  Husband, Dad, Wake County Commissioner. Former...              617   \n",
       "4  Living the Colorado Life! + Crypto Enthusiast!...              131   \n",
       "\n",
       "   organization  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "user_label_data_clean = pd.read_csv('../../data/mined_tweets/tweet_user_data/user_data_media_label_clean.tsv', sep='\\t', index_col=False)\n",
    "user_label_data_clean.fillna('', inplace=True)\n",
    "display(user_label_data_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_labels(data_label, data_gold, label_var='organization'):\n",
    "    # score precision/recall on org labels\n",
    "    tp_data = (data_label.loc[:, label_var] - data_gold.loc[:, label_var]==0) & (data_label.loc[:, label_var]==1)\n",
    "    fp_data = (data_label.loc[:, label_var] - data_gold.loc[:, label_var] == 1)\n",
    "    fn_data = (data_label.loc[:, label_var] - data_gold.loc[:, label_var] == -1)\n",
    "    tp = tp_data.sum()\n",
    "    fp = fp_data.sum()\n",
    "    fn = fn_data.sum()\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    return tp_data, fp_data, fn_data, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "organization labels: prec=0.871, rec=0.535\n"
     ]
    }
   ],
   "source": [
    "author_var = 'username'\n",
    "label_var = 'organization'\n",
    "data_label = org_data_label.loc[:, [author_var, 'label_score', label_var]]\n",
    "data_gold = user_label_data_clean.loc[:, [author_var, label_var]]\n",
    "# limit to shared authors\n",
    "shared_users = set(data_label.loc[:, author_var].unique()) & set(data_gold.loc[:, author_var].unique())\n",
    "data_label = data_label[data_label.loc[:, author_var].isin(shared_users)]\n",
    "data_gold = data_gold[data_gold.loc[:, author_var].isin(shared_users)]\n",
    "# drop duplicate authors\n",
    "data_gold.drop_duplicates(author_var, inplace=True)\n",
    "data_label.drop_duplicates(author_var, inplace=True)\n",
    "# same order\n",
    "data_gold.sort_values(author_var, inplace=True, ascending=True)\n",
    "data_label.sort_values(author_var, inplace=True, ascending=True)\n",
    "data_label.index = data_gold.index\n",
    "tp_data, fp_data, fn_data, prec, rec = score_labels(data_label, data_gold, label_var=label_var)\n",
    "print('organization labels: prec=%.3f, rec=%.3f'%(prec, rec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false positives\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['airlivenet', 'corkgaines', 'edison_electric', 'figment_imagine',\n",
       "       'noircoffeegroup', 'sjearthquakes', 'spencerweather',\n",
       "       'ucf_marcdaniels'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "false negatives\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['1025wfmf', 'allhandsdotnet', 'alt1053', 'americares',\n",
       "       'asicentral', 'austinpetsalive', 'awhonn', 'bbb_enterprise',\n",
       "       'bethuneformb', 'bridge_golf', 'chambanawx', 'chicagosmayor',\n",
       "       'chrisdisaster', 'cincyshrimp', 'clemprograms', 'complyethic',\n",
       "       'computercareon', 'conedison', 'dpltoday', 'earth_hazards',\n",
       "       'enlacanchapr', 'fox_chattanooga', 'georgiapower',\n",
       "       'gerdasequineres', 'glazingblogger', 'goexpresstruck',\n",
       "       'heart911team', 'hhazelwoodesq', 'hibbertgroupre', 'humanesociety',\n",
       "       'kbtrealtyteam', 'lemondnutrition', 'livemapus', 'loopnewsbb',\n",
       "       'matt_barrentine', 'mcsweeneycac', 'mtzionnashville',\n",
       "       'myarklamiss', 'ncweatherhound', 'nwswpc', 'pinksugarmiami',\n",
       "       'qtweather', 'specifiermagcsi', 'tachiractiva', 'wbbj7weather',\n",
       "       'wvtm13', 'xwnetwork'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show false positive examples\n",
    "print('false positives')\n",
    "display(data_gold[fp_data].loc[:, author_var].unique())\n",
    "# show false negative examples\n",
    "print('false negatives')\n",
    "display(data_gold[fn_data].loc[:, author_var].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem like there's a consistent bias toward certain types of accounts that are misidentified (mix of weather, news, government accounts). This is a good sign. We can use the classifier to get high-precision data on organizations and then compare rates of context use among organizations and non-organizations, in the same way that we compared locals and non-locals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
